{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,6,3)\n",
    "        self.conv2 = nn.Conv2d(6,16,3)\n",
    "        self.linear1 = nn.Linear(16*6*6,120)\n",
    "        self.linear2 = nn.Linear(120,84)\n",
    "        self.linear3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self,image):\n",
    "        image = self.conv1(image).relu()\n",
    "        image = F.max_pool2d(image,(2,2))\n",
    "        image = self.conv2(image).relu()\n",
    "        image = F.max_pool2d(image,(2,2))\n",
    "\n",
    "        image = image.view(-1, self.num_flat_features(image))\n",
    "        image = F.relu(self.linear1(image))\n",
    "        image = F.relu(self.linear2(image))\n",
    "        image = self.linear3(image)\n",
    "        return F.log_softmax(image)\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create LeNet Model \n",
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (linear1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (linear2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (linear3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Show the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "Output:  tensor([[-2.1437, -2.4591, -2.2105, -2.3772, -2.4137, -2.2888, -2.2812, -2.2778,\n",
      "         -2.2790, -2.3342]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/c7ch578s2k7_h10mbq482nb00000gp/T/ipykernel_1167/1946341736.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(image)\n"
     ]
    }
   ],
   "source": [
    "### Passing a image  of random 32*32 pixels\n",
    "image = torch.rand((1,1,32,32))\n",
    "print(image.shape)\n",
    "\n",
    "print(\"Output: \",model(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "####### Load cifar10 data\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "transforms = transforms.Compose([transforms.ToTensor(),\n",
    "transforms.Grayscale(),transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/c7ch578s2k7_h10mbq482nb00000gp/T/ipykernel_1167/1946341736.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(image)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.052\n",
      "[1,   200] loss: 0.053\n",
      "[1,   300] loss: 0.053\n",
      "[2,   100] loss: 0.294\n",
      "[2,   200] loss: 0.287\n",
      "[2,   300] loss: 0.292\n",
      "[3,   100] loss: 0.258\n",
      "[3,   200] loss: 0.262\n",
      "[3,   300] loss: 0.264\n",
      "[4,   100] loss: 0.203\n",
      "[4,   200] loss: 0.202\n",
      "[4,   300] loss: 0.199\n",
      "[5,   100] loss: 0.109\n",
      "[5,   200] loss: 0.110\n",
      "[5,   300] loss: 0.109\n",
      "[6,   100] loss: 0.107\n",
      "[6,   200] loss: 0.107\n",
      "[6,   300] loss: 0.107\n",
      "[7,   100] loss: 0.111\n",
      "[7,   200] loss: 0.111\n",
      "[7,   300] loss: 0.111\n",
      "[8,   100] loss: 0.113\n",
      "[8,   200] loss: 0.112\n",
      "[8,   300] loss: 0.113\n",
      "[9,   100] loss: 0.114\n",
      "[9,   200] loss: 0.114\n",
      "[9,   300] loss: 0.114\n",
      "[10,   100] loss: 0.114\n",
      "[10,   200] loss: 0.113\n",
      "[10,   300] loss: 0.114\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i,train_batch in enumerate(trainloader,0):\n",
    "        images, labels = train_batch\n",
    "        \n",
    "        preds = model(images)\n",
    "        loss = criterion(preds,labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transforms)\n",
    "testloader = torch.utils.data.DataLoader(trainset,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAb3UlEQVR4nO2dW2ylV3XH/+vc7bHHnqs9t2SSMIFEkJtMiAhNaWlpQEghSKTwgPIQMTwQqUj0IUqrkvYJqgJCaoU0lJS0okDUECWiESWaAiltCXHSXCaZhFyYJJOZzM0zvtvntvpwTqpJ+v2X7WP7eMj+/yTLx3ud/e119vet8x3v/1lrm7tDCPH2J7fWDgghuoOCXYhEULALkQgKdiESQcEuRCIo2IVIhMJyOpvZ9QC+ASAP4O/d/cvR80uFXu8pDS55HDfqQeDckofpGGt2Jl9W1+c76lecanIjk1ItmJBAfrXopQX9vJB9H/Hcyp8zq/H5sAbxsV6nfbzZ6MyPXHA+izzU2FxFODmfc3OnUatOZxo7DnYzywP4OwB/COAwgEfM7H53f4b16SkN4pp3fXbJY3meXAU5Pkn8DaJzPJ89Xn6myjuxiw3Aax/eyPsF/m/7j0nejVz4XuYXotWDYIlsNR4UtY292e19wSXX4TmrHJ+lttwEsZ0Yo32ak3x+I3J967hxxzA11Qd7Mtuja7hRyT6fj/7yb2mf5XyMvxrAC+7+krtXAXwfwA3LOJ4QYhVZTrDvAPDqWX8fbrcJIc5BlhPsWR8y/t9nVjPba2ajZjZarc8sYzghxHJYTrAfBrDrrL93Ajjy1ie5+z53H3H3kVIh+/84IcTqs5xgfwTAHjO7wMxKAD4F4P6VcUsIsdJ0vBrv7nUzuxXAv6Elvd3p7k8v2LFJVncjaahJbOxYAIyt4ANoFoOV6UZwTCI1NXpLtA8CqWnD81z+aZR4v0jqYz4iWDlnKgMAIJqr4JjFsex/2ZqlPtqnWeCvufL6NLX5sy/xYzayfYxWzvNbNlMbKmVqqu7k6kpt/dJDLTcfSKwdsCyd3d0fAPDACvkihFhF9A06IRJBwS5EIijYhUgEBbsQiaBgFyIRlrUav1TcAM9nSzlRogaThnLVSE7qLLuqWQ6yk8gxc0HWVZQZVh4LEmgCoiypZonMbyAB0swwZHwl8ux+kSw3m/3a8nOBXDdT44M9/zI15TcMUltzeFNme72nSPtE81sd5P3qlWCOg4lkCS+5EvcjP8ckbD6O7uxCJIKCXYhEULALkQgKdiESQcEuRCJ0dTXePEjUiFaEc9m26gBPQIlWVD0q/RasmuaqpOQTURgAIBeUdcrN85XpRiVQBUhJopaRtEcln4LXnJ/lyTrNwMe5nSThJfCjHCgX+Yt3U9sUGwtAbV32dRCtnHtwC8wF5elyNe5/1I+txtezq1UBABpEvfIgmUh3diESQcEuRCIo2IVIBAW7EImgYBciERTsQiTCGiTCZEsDnezgkqsHcp1zyasR7CTDpBoAKBDfCzNBQk603VFQS47VcAOA+gDXZJj04h1ulVXv54kfs5v55TO7KXseLZCgSoNBMlRwW2pyF2HkMpgZ7kx6K00EYzX4MUsT/FyXpomTQV4Q9TGSjrlJCPF2QsEuRCIo2IVIBAW7EImgYBciERTsQiTCsqQ3MzsEYBJAA0Dd3UfCDjlDI6j9FQyU3RxkykVyUlSfLtqCyPPZ40XHi7ZqCjPzynyemsWgHzHlgzp5Hmy9NXE+zyycGQrmkXSrnKJd0Ah20YrktWYxmn8yFt/FCc1ysL0W24oMWKBgXyR9EpkyunbI8SLZcCV09t9z95MrcBwhxCqij/FCJMJyg90B/MTMHjWzvSvhkBBidVjux/hr3f2ImW0F8KCZPevuD539hPabwF4AKJcHljmcEKJTlnVnd/cj7d/HAdwL4OqM5+xz9xF3HykV+Z7YQojVpeNgN7N1Ztb/xmMAHwZwYKUcE0KsLMv5GD8E4F5rSQAFAP/s7j8Oe5hxuSlSNCLZgo4VHC+o18ikmlZH0h5IJJF80ujh01/r7+A1A6gOZB8zkikbZT7W9PbOsuUqJ5beJ3rN+bmgmGNQ6LFZIrJtIJM1C51Jb1FGX1Q8kl3fuSDrjfm/KtKbu78E4PJO+wshuoukNyESQcEuRCIo2IVIBAW7EImgYBciEbpacBIAlV6iwoxRVhntE3TJVYNCj41gbzYi8UQyTq032LMteF25IEstktFOvif7mNUt/Hi9L3Mtst7Lx+o/RE3oGcseb3IHH4tlygFAfp7bIsmr2cEV7kGf6HjFwMdmIPeyDLwc32YPOTJWJB3rzi5EIijYhUgEBbsQiaBgFyIRFOxCJEJ3t39CZ0ktbGU9rEEX1HeLiJIqrJ691NmoBGNF2/EEK+6RH7V+vrRbuDR7f6LzBvm+RS9Wt1NbeYyPNfASX35mq8LTW/nxGhVqiq+bXFT8jR1w6V0AoDrIz1ltfZTIw49ZmM7uFykQpalsP7QaL4RQsAuRCgp2IRJBwS5EIijYhUgEBbsQidBV6c3ckasGWQuEXDVbmoi2QbIokSSoC9eMtoYi40WJGBHhNlRBv2off92/e96Lme2NIDPopfoOattwkHtSOs31pEZfdlZLtI1TJL1FMlQoa81mn2tWmw4APJDyPJIHc3yuGj2RLEfkyEAerPdkXwNRDTrd2YVIBAW7EImgYBciERTsQiSCgl2IRFCwC5EIC0pvZnYngI8BOO7u7263bQTwAwC7ARwCcJO7n15wNOeZanRbqID8HC/SxTLUAKBOZCEAaPTwrCwma0TSWyTzRXX3ItllZpjP1XXrn8ts/+XURbRPYYb7UR4PCqFFmWhk+sOsrCq3MQkNAJqFQMIkpzqS8qItqiJNtDAV7SsWHJLIkbU+3qlCavyFWZbc9H98B8D1b2m7DcB+d98DYH/7byHEOcyCwd7eb33sLc03ALir/fguAB9fWbeEECtNp/+zD7n7UQBo/966ci4JIVaDVV+gM7O9ZjZqZqO12vRqDyeEIHQa7MfMbBsAtH8fZ090933uPuLuI8Xiug6HE0Isl06D/X4AN7cf3wzgvpVxRwixWixGevsegA8C2GxmhwF8CcCXAdxtZrcAeAXAJxc1mnGJLcpg45k8UYpPIMcE2WZWDwpO8tECP7gpLJgZMLWba32Xlo9mtv90/F20D8sqBGIJsxMi6S3KepvbxH2c2RnIgz3Zc9XzAtlzCUBhip+03iPcj97j/MXNbOXHrPVRE4XKtsFFumCwu/unielDi/BJCHGOoG/QCZEICnYhEkHBLkQiKNiFSAQFuxCJ0N293szQLGW/vzRIOwA4yWqKsp2ibLNcNZDXAjXMWV3AIPsr8iM/z6WauU381NgGnh72bHUos/3X4/wbzcXgi42dZCMCQK6WLXmVx/lrntrF57G+Z5bahganqC1PikCOPzfMj/cr7mPPMV7dslniWW+1Xi71Fbn7/Hjk+2kqOCmEULALkQoKdiESQcEuRCIo2IVIBAW7EInQ3b3emo78bHaGkjW5bMFkOc9HKWWBI0G3ZiCjNYvZNibJAQsUSgzkxqnt/KCVHi693Xn4A5nth57PluQAYH2QNDa7KSjAaT3cRjILoyKbkexZrtSobXMv1w43lrNtz5zi89H/HK+d2uzlxUqrg0EhU668IUeSGBvkegNiSZeOs+QeQojfShTsQiSCgl2IRFCwC5EICnYhEqGrq/FwR66avfSYq/HkgxxLxrBo9ZOvZIbbLgXkWH06vlAcJslM7eIr3TM7+GprY5ov7T43vi2z3Wr8fX1+U5A0FPSrrg+UC5KkVO+lXVDv5dfAuzadorZrNvyG2k6RjJHeE7yOn43zzBTv20RtZ95B9nECUJrgc1zrJfMYqUasjmJwaevOLkQiKNiFSAQFuxCJoGAXIhEU7EIkgoJdiERYzPZPdwL4GIDj7v7udtsdAD4L4ET7abe7+wPLcaTZQa2zaGuinAU17YLtnzyYkVyNyCdBTsLUDn7AiQt4v8Y6/tpsjEuOvi5bUtr2Trr3Jo4c2syPlwsSYYJT1nMiuz1KGvI+npFzXi9PTvmj/qeo7SuvfSSzvfc1XtMOUTJUT3A+93A5r3KMv/AyeWl1nmeEHJkqX6b09h0A12e0f93dr2j/LCvQhRCrz4LB7u4PARjrgi9CiFVkOf+z32pmT5rZnWa2YcU8EkKsCp0G+zcBXATgCgBHAXyVPdHM9prZqJmN1uozHQ4nhFguHQW7ux9z94a7NwF8C8DVwXP3ufuIu48UC8EXo4UQq0pHwW5mZ2db3AjgwMq4I4RYLRYjvX0PwAcBbDazwwC+BOCDZnYFWqLTIQCfW9RoZjzjLEpEI1KIcXUqfhuL5Ilgaygmsc1t5LLK+EX8cPUNXGoqjPNTUx6Lss2yX/hH3vcM7bM//05q23HZOLU9fGg3tc0UsnWjwlQw+XVue32un9qent9ObY+O7slsv3j2DO3T2DJIbdPDPOPQy1x6mw2yGEvkXFfX8z5OEuwi6XjBYHf3T2c0f3uhfkKIcwt9g06IRFCwC5EICnYhEkHBLkQiKNiFSITuFpwE37IpKgLptOAkH4cVPFzIVpjhckd9XbYfk+fx98zmTp5dlc8FhR5P8FNT6+P98rPZr+03Mzyz7aqNr1LbJzaMUtvhqU9Q28tzxH+mGQGwKp/H509toba75t9PbcP/mT1X5nwO54b4l7+mtwX3R+fSGwpBwcm+7PZ8lV+ncxuyx4qyCnVnFyIRFOxCJIKCXYhEULALkQgKdiESQcEuRCJ0V3ozAERi8yJ/32mUs/WESGaIZLm44CS3TQ9lDzi7jUsupRLPbMvnedrezBCXqC6/+BVqe+Lg+ZntP3/pHbTPTZc8Rm1//uKN1Hbsv3i22TqiOM5v4BJUYYpfAxPHiT4FYOJU9n5uALDntbnMdi/yi6feE1yLFWpC4TQPp0awj12VzElxvLM9CRm6swuRCAp2IRJBwS5EIijYhUgEBbsQidDl1XhDk6y6s/aWjazgR1sJBYk1CJIg6hXeb2aYJFVsqNI+zQZ/XcEuQ4BxH0/O8tXn3Ez2eOWXeXLH9+bfS22V3/Caa6VJakJ1fXZ7fYArF1EiTFSfru/XXLnIT0xltjd7eZ9qf6QMUROapaB+4WCNmuq17PEKU3ybLzTIfAQu6M4uRCIo2IVIBAW7EImgYBciERTsQiSCgl2IRFjM9k+7APwjgGEATQD73P0bZrYRwA8A7EZrC6ib3P10dCzPAfVektQSSGWsZpwHb1XR1lCBqoWZLfyg1S3ZstHGwWnaJ6ozN1vl8s+GrVzXWl/OTu4AWlvqZvoxT7ug/CqXeIpX8VM63M99rDazz/Phx3jyjAUl3Gpb+DyWT0fbJGX70Sxx3bbGlU00i9FYgW2Oj2dEesvxHCoUJ8iWaMEcLubOXgfwRXe/BMA1AD5vZpcCuA3AfnffA2B/+28hxDnKgsHu7kfd/bH240kABwHsAHADgLvaT7sLwMdXyUchxAqwpP/ZzWw3gCsBPAxgyN2PAq03BABbV9w7IcSKsehgN7M+APcA+IK7Tyyh314zGzWz0VqV/28rhFhdFhXsZlZEK9C/6+4/bDcfM7Ntbfs2AMez+rr7PncfcfeRYilY+RBCrCoLBruZGVr7sR9096+dZbofwM3txzcDuG/l3RNCrBSLyXq7FsBnADxlZo+3224H8GUAd5vZLQBeAfDJxQxIJbYwA2wxR35LlyCzrcG2kwIwOxRsDbU+O7utUuQayUAgk00VeArV8Dr+n9LdF+6ntqsm/zh7rOpG2mfXyGvUFvE7W16gtvteuSyzvXSGz28koc1Nc5ly3bFAoyLnulGJatAFMnCQ2ealQO8tcpuTraGq6/l1WmLSW+DCgsHu7r8AD7cPLdRfCHFuoG/QCZEICnYhEkHBLkQiKNiFSAQFuxCJ0N2CkwBd1w+3clrisVoH5Kb5Ad6x1h9oF4Smd7ZNT5S9dmqOfwHpl3M8tem8gTOZ7dd97BHaZ3uRZ7b9w+Frqe3x8Z3UdvLoQGZ7Jaih2HOKz/3GZ3nBxuJpstcUgEZv9oATF3BHZnYE18AwTx8sBOmUjSq/wHPlbOmwPsjvxYWZ7NCNLkXd2YVIBAW7EImgYBciERTsQiSCgl2IRFCwC5EI3ZfeCM18oBkwUyCvRdk/84NBVlOUnTSXPV2TsxXap5znMlmlwOWksWm+N9s9Z0ao7a/OX3qm8Y8mLqe2iSrPzKtHFT9r2XPcc4yftMoYz14rHeVZgF7kl/HYpdnzOLmbdkFzM9+7rxRkOM6P9fB+p7j01iQJfVZZeqZcJEfrzi5EIijYhUgEBbsQiaBgFyIRFOxCJMI5sxofEa2sM5pFvizZ4AvM8KDGmOWzHZmb5UkVk0W+4h4l0DQCWznYF2iYrP7/ePp82ucXpy6itvkav0Rm5vnrZlsalSaDbZyO8u2k0OQXQXWIJw2dfk/2eGz7JADAGf665qf5fLDXDKC1cRqhcjLbl0Y52DKKXAJRrOjOLkQiKNiFSAQFuxCJoGAXIhEU7EIkgoJdiERYUHozs10A/hHAMFoCwj53/4aZ3QHgswBOtJ96u7s/EB3LrcNac0ytCdSTuQH+Pja/lSen9A9z+SdPaozNzAX1zOa4ztdsch+LQQJNf57XrntkflNm+1ijj/Yp5fhYBSI3AsDUDH9tTALKVwNpcyLY5bfBfczP9FNbaSzbx2IwVI4k8QDAPN9FC80yn6tGDz/XPp09Xo6rtohykBiL0dnrAL7o7o+ZWT+AR83swbbt6+7+N0sfVgjRbRaz19tRAEfbjyfN7CCAHavtmBBiZVnShwEz2w3gSgAPt5tuNbMnzexOM9uw0s4JIVaORQe7mfUBuAfAF9x9AsA3AVwE4Aq07vxfJf32mtmomY3W54N/lIQQq8qigt3MimgF+nfd/YcA4O7H3L3h7k0A3wJwdVZfd9/n7iPuPlIo8+8wCyFWlwWD3cwMwLcBHHT3r53Vvu2sp90I4MDKuyeEWCkWsxp/LYDPAHjKzB5vt90O4NNmdgVawtghAJ9b+FAGt2yZoRMpIWJ2iMsnvdunqO3CDWPU1lfM3vrn2CyXfo5NcskryiirVbnteJWPN1nKroNWCzTPqBbeyUNca6q8zo/ZT2rN9Zzg9d1CcoFMefgUtZ3/r9npYbUBXjfw9fdxSbFyHpdmPchUnB3n4803sovQ5ef48VgNukjaXsxq/C+QrWiHmroQ4txC36ATIhEU7EIkgoJdiERQsAuRCAp2IRKh+wUniZpAEspCWzOQGWp9/ICNObLfDoCXz/Bv/V6y+Vhm+5YKl/IiXp/gEtos2WoKAH525B3UVtuWPSlPjvF0hsMnB6nNqsFWWcHVUyCJecWTM0Gn6ITyIps+xb+Zmc9l+58/Pk77DNsQtb26fj21ld7Jt6gq9nJ5s0YyNBt9fO7zk9l9gjDSnV2IVFCwC5EICnYhEkHBLkQiKNiFSAQFuxCJcO7s9RZpBoRIrht4ntvGLTszDADObONT8kQ9Wxraup5LbwMlXhyyr5KdRQcA0wWeJTV5ILuoJAA8MNmb2d6/jvthr/L5KM5w+acQ1CIpT5ACkYXg/kIyIgHA64H0NjvLD1khGWxlXiS0/MpparvgXl6T4fX3D1Bb/So+/8iRDLbgAm+WmIYdDMNNQoi3Ewp2IRJBwS5EIijYhUgEBbsQiaBgFyIRuiu9WWu/txUjkN76jvIso9IUz646fTGXZGZJRtkRIskBQG4Td7JS4HLS0BaelXXixBZqKz2TLb1N93J5rRAUNsxzdRDF6WDfNvLSWMFRAPAZLqFhNpKuon3USJZdk+/LhhLPRsxP8QnZ/u/8mjt1mmdTnr4ku71Z4fPLCk5KehNCKNiFSAUFuxCJoGAXIhEU7EIkwoKr8WZWAfAQgHL7+f/i7l8ys40AfgBgN1rbP93k7jyDAAAcMLYIGq3SL33hMVz1L8zwldj+V3hHViusejrYSqjAV3a3D/KaZVdufo0f872834tjmzPb88GETB3lW1RtfIIrDeteJ8kuAErj2SvT5sEKfrCq3gz65dbzOfa57FX8xrHjtE8hUAwau7ZSm83y1fgtP+fnszS9LbP95GV8Pmr8JVMWc2efB/D77n45WtszX29m1wC4DcB+d98DYH/7byHEOcqCwe4t3sjhLLZ/HMANAO5qt98F4OOr4aAQYmVY7P7s+fYOrscBPOjuDwMYcvejAND+zT/fCCHWnEUFu7s33P0KADsBXG1m717sAGa218xGzWy0Nt9ZfXUhxPJZ0mq8u58B8DMA1wM4ZmbbAKD9O3PFw933ufuIu48Uy3whSAixuiwY7Ga2xcwG2497APwBgGcB3A/g5vbTbgZw3yr5KIRYARaTCLMNwF1mlkfrzeFud/+Rmf03gLvN7BYArwD45MKHci69RNs/EYXHo7eqKOEiz20zw4HcMUSSIOa4PDUzlp2YAgBnylyqeRHZEhoATNV4sk6TSGxzs7xP3yF+GVROBwkjAbX+pedYlc5wH40ltADwGp/HJkmgsQL3rzm0kdrq6/jWYbmgvp7N8QSagdGjme3VPr5l1+lLyDUcnK4Fz4i7Pwngyoz2UwA+tFB/IcS5gb5BJ0QiKNiFSAQFuxCJoGAXIhEU7EIkgnmQTbTig5mdAPBy+8/NAE52bXCO/Hgz8uPN/Lb5cb67ZxYp7Gqwv2lgs1F3H1mTweWH/EjQD32MFyIRFOxCJMJaBvu+NRz7bOTHm5Efb+Zt48ea/c8uhOgu+hgvRCKsSbCb2fVm9pyZvWBma1a7zswOmdlTZva4mY12cdw7zey4mR04q22jmT1oZs+3f/P9glbXjzvM7LX2nDxuZh/tgh+7zOynZnbQzJ42sz9pt3d1TgI/ujonZlYxs1+Z2RNtP/6y3b68+XD3rv4AyAN4EcCFAEoAngBwabf9aPtyCMDmNRj3OgBXAThwVttfA7it/fg2AF9ZIz/uAPCnXZ6PbQCuaj/uB/BrAJd2e04CP7o6J2gVTu5rPy4CeBjANcudj7W4s18N4AV3f8ndqwC+j1bxymRw94cAjL2luesFPIkfXcfdj7r7Y+3HkwAOAtiBLs9J4EdX8RYrXuR1LYJ9B4BXz/r7MNZgQts4gJ+Y2aNmtneNfHiDc6mA561m9mT7Y/6q/ztxNma2G636CWta1PQtfgBdnpPVKPK6FsGeVWJjrSSBa939KgAfAfB5M7tujfw4l/gmgIvQ2iPgKICvdmtgM+sDcA+AL7g73wmj+350fU58GUVeGWsR7IcB7Drr750AjqyBH3D3I+3fxwHci9a/GGvFogp4rjbufqx9oTUBfAtdmhMzK6IVYN919x+2m7s+J1l+rNWctMc+gyUWeWWsRbA/AmCPmV1gZiUAn0KreGVXMbN1Ztb/xmMAHwZwIO61qpwTBTzfuJja3IguzImZGYBvAzjo7l87y9TVOWF+dHtOVq3Ia7dWGN+y2vhRtFY6XwTwZ2vkw4VoKQFPAHi6m34A+B5aHwdraH3SuQXAJrS20Xq+/XvjGvnxTwCeAvBk++La1gU/PoDWv3JPAni8/fPRbs9J4EdX5wTAZQD+pz3eAQB/0W5f1nzoG3RCJIK+QSdEIijYhUgEBbsQiaBgFyIRFOxCJIKCXYhEULALkQgKdiES4X8B08QsgdLTEpMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Classes=['plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for test_image in testloader:\n",
    "    print(\"Label: \",Classes[label[0]])\n",
    "    image,label = test_image\n",
    "    plt.imshow(image[0].squeeze(0))\n",
    "    plt.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/c7ch578s2k7_h10mbq482nb00000gp/T/ipykernel_1167/1946341736.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(image)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 63 %\n"
     ]
    }
   ],
   "source": [
    "total=0\n",
    "correct = 0\n",
    "for test_batch in testloader:\n",
    "    with torch.no_grad():\n",
    "        image,label = test_batch\n",
    "        preds = model(image)\n",
    "        _, predicted = torch.max(preds.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('imageproc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbf8f6931e593c1d20f304ee88c30b2bf8792293dec0efad3793d777ab79c16d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
