{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(MLP,self).__init__()\n",
    "\n",
    "        self.w00 = nn.Parameter(torch.tensor(0.22),requires_grad=True)\n",
    "        self.w01 = nn.Parameter(torch.tensor(0.05),requires_grad=True)\n",
    "        self.b0 = nn.Parameter(torch.tensor(-0.05),requires_grad=True)\n",
    "\n",
    "        self.w100 = nn.Parameter(torch.tensor(0.01),requires_grad=True)\n",
    "        self.w101 = nn.Parameter(torch.tensor(0.01),requires_grad=True)\n",
    "        self.w110 = nn.Parameter(torch.tensor(0.01),requires_grad=True)\n",
    "        self.w111 = nn.Parameter(torch.tensor(0.01),requires_grad=True)\n",
    "        self.b1 = nn.Parameter(torch.tensor(0.05),requires_grad=True)\n",
    "\n",
    "        self.w20 = nn.Parameter(torch.tensor(0.05),requires_grad=True)\n",
    "        self.w21 = nn.Parameter(torch.tensor(0.05),requires_grad=True)\n",
    "        self.b2 = nn.Parameter(torch.tensor(0.05),requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        u10 = F.relu(x*self.w00+self.b0)\n",
    "        u11 = F.relu(x*self.w01+self.b0)\n",
    "\n",
    "        u20 = F.relu(u10*self.w100+u11*self.w110+self.b1)\n",
    "        u21 = F.relu(u10*self.w101+u11*self.w111+self.b1)\n",
    "\n",
    "        final = F.relu(u20*self.w20+u21*self.w21+self.b2)\n",
    "\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Simulate Y = 3*X^2\n",
    "\n",
    "X = torch.unsqueeze(torch.linspace(-100, 100, 10000), dim=1)\n",
    "Y = 3*X**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: list of x variables\n",
    "            Y: list of y variables\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.index_select(self.X, 0, torch.tensor(idx))\n",
    "        y = torch.index_select(self.Y, 0, torch.tensor(idx))\n",
    "        sample = (x,y)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PolyDataset(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(dataset,batch_size=1024,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    \n",
    "    for batch in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        x,y = batch\n",
    "        preds = model(x)\n",
    "        loss = criterion(y,preds).sqrt()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: \",loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([1.0,1.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-70.4370]],\n",
      "\n",
      "        [[ 11.2311]],\n",
      "\n",
      "        [[ 85.5386]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 66.6967]],\n",
      "\n",
      "        [[ 90.1190]],\n",
      "\n",
      "        [[ 21.5522]]])\n"
     ]
    }
   ],
   "source": [
    "for batch in trainloader:\n",
    "    x,y = batch\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.0536], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPTorch(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(MLPTorch,self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(1,32)\n",
    "        self.linear2 = nn.Linear(32,64)\n",
    "        self.linear3 = nn.Linear(64,64)\n",
    "        self.linear4 = nn.Linear(64,32)\n",
    "        self.linear5 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x).relu()\n",
    "        x = self.linear2(x).relu()\n",
    "        x = self.linear3(x).relu()\n",
    "        x = self.linear4(x).relu()\n",
    "        x = self.linear5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss:  tensor(13597.2217, grad_fn=<SqrtBackward0>)\n",
      "Epoch 2, Training Loss:  tensor(13760.2881, grad_fn=<SqrtBackward0>)\n",
      "Epoch 3, Training Loss:  tensor(13596.1904, grad_fn=<SqrtBackward0>)\n",
      "Epoch 4, Training Loss:  tensor(13204.8057, grad_fn=<SqrtBackward0>)\n",
      "Epoch 5, Training Loss:  tensor(13410.2344, grad_fn=<SqrtBackward0>)\n",
      "Epoch 6, Training Loss:  tensor(12893.1123, grad_fn=<SqrtBackward0>)\n",
      "Epoch 7, Training Loss:  tensor(12342.9854, grad_fn=<SqrtBackward0>)\n",
      "Epoch 8, Training Loss:  tensor(10794.9980, grad_fn=<SqrtBackward0>)\n",
      "Epoch 9, Training Loss:  tensor(8833.3887, grad_fn=<SqrtBackward0>)\n",
      "Epoch 10, Training Loss:  tensor(5679.3022, grad_fn=<SqrtBackward0>)\n",
      "Epoch 11, Training Loss:  tensor(4044.6589, grad_fn=<SqrtBackward0>)\n",
      "Epoch 12, Training Loss:  tensor(3470.2327, grad_fn=<SqrtBackward0>)\n",
      "Epoch 13, Training Loss:  tensor(3474.2234, grad_fn=<SqrtBackward0>)\n",
      "Epoch 14, Training Loss:  tensor(3428.6294, grad_fn=<SqrtBackward0>)\n",
      "Epoch 15, Training Loss:  tensor(3421.9744, grad_fn=<SqrtBackward0>)\n",
      "Epoch 16, Training Loss:  tensor(3406.7556, grad_fn=<SqrtBackward0>)\n",
      "Epoch 17, Training Loss:  tensor(3408.7900, grad_fn=<SqrtBackward0>)\n",
      "Epoch 18, Training Loss:  tensor(3422.3828, grad_fn=<SqrtBackward0>)\n",
      "Epoch 19, Training Loss:  tensor(3321.0608, grad_fn=<SqrtBackward0>)\n",
      "Epoch 20, Training Loss:  tensor(3417.4871, grad_fn=<SqrtBackward0>)\n",
      "Epoch 21, Training Loss:  tensor(3381.4460, grad_fn=<SqrtBackward0>)\n",
      "Epoch 22, Training Loss:  tensor(3285.9792, grad_fn=<SqrtBackward0>)\n",
      "Epoch 23, Training Loss:  tensor(3340.4734, grad_fn=<SqrtBackward0>)\n",
      "Epoch 24, Training Loss:  tensor(3458.1172, grad_fn=<SqrtBackward0>)\n",
      "Epoch 25, Training Loss:  tensor(3293.0125, grad_fn=<SqrtBackward0>)\n",
      "Epoch 26, Training Loss:  tensor(3378.7031, grad_fn=<SqrtBackward0>)\n",
      "Epoch 27, Training Loss:  tensor(3324.2751, grad_fn=<SqrtBackward0>)\n",
      "Epoch 28, Training Loss:  tensor(3398.0706, grad_fn=<SqrtBackward0>)\n",
      "Epoch 29, Training Loss:  tensor(3325.0002, grad_fn=<SqrtBackward0>)\n",
      "Epoch 30, Training Loss:  tensor(3276.7578, grad_fn=<SqrtBackward0>)\n",
      "Epoch 31, Training Loss:  tensor(3309.6609, grad_fn=<SqrtBackward0>)\n",
      "Epoch 32, Training Loss:  tensor(3341.3462, grad_fn=<SqrtBackward0>)\n",
      "Epoch 33, Training Loss:  tensor(3395.2092, grad_fn=<SqrtBackward0>)\n",
      "Epoch 34, Training Loss:  tensor(3316.2583, grad_fn=<SqrtBackward0>)\n",
      "Epoch 35, Training Loss:  tensor(3232.0220, grad_fn=<SqrtBackward0>)\n",
      "Epoch 36, Training Loss:  tensor(3335.1160, grad_fn=<SqrtBackward0>)\n",
      "Epoch 37, Training Loss:  tensor(3388.1914, grad_fn=<SqrtBackward0>)\n",
      "Epoch 38, Training Loss:  tensor(3290.6912, grad_fn=<SqrtBackward0>)\n",
      "Epoch 39, Training Loss:  tensor(3316.3357, grad_fn=<SqrtBackward0>)\n",
      "Epoch 40, Training Loss:  tensor(3141.6907, grad_fn=<SqrtBackward0>)\n",
      "Epoch 41, Training Loss:  tensor(3292.6348, grad_fn=<SqrtBackward0>)\n",
      "Epoch 42, Training Loss:  tensor(3309.6104, grad_fn=<SqrtBackward0>)\n",
      "Epoch 43, Training Loss:  tensor(3243.2180, grad_fn=<SqrtBackward0>)\n",
      "Epoch 44, Training Loss:  tensor(3317.1514, grad_fn=<SqrtBackward0>)\n",
      "Epoch 45, Training Loss:  tensor(3257.1465, grad_fn=<SqrtBackward0>)\n",
      "Epoch 46, Training Loss:  tensor(3266.9829, grad_fn=<SqrtBackward0>)\n",
      "Epoch 47, Training Loss:  tensor(3354.1436, grad_fn=<SqrtBackward0>)\n",
      "Epoch 48, Training Loss:  tensor(3182.7883, grad_fn=<SqrtBackward0>)\n",
      "Epoch 49, Training Loss:  tensor(3238.5884, grad_fn=<SqrtBackward0>)\n",
      "Epoch 50, Training Loss:  tensor(3287.8474, grad_fn=<SqrtBackward0>)\n",
      "Epoch 51, Training Loss:  tensor(3196.8323, grad_fn=<SqrtBackward0>)\n",
      "Epoch 52, Training Loss:  tensor(3244.7986, grad_fn=<SqrtBackward0>)\n",
      "Epoch 53, Training Loss:  tensor(3259.6953, grad_fn=<SqrtBackward0>)\n",
      "Epoch 54, Training Loss:  tensor(3290.8396, grad_fn=<SqrtBackward0>)\n",
      "Epoch 55, Training Loss:  tensor(3189.1626, grad_fn=<SqrtBackward0>)\n",
      "Epoch 56, Training Loss:  tensor(3220.6838, grad_fn=<SqrtBackward0>)\n",
      "Epoch 57, Training Loss:  tensor(3066.9351, grad_fn=<SqrtBackward0>)\n",
      "Epoch 58, Training Loss:  tensor(3174.8618, grad_fn=<SqrtBackward0>)\n",
      "Epoch 59, Training Loss:  tensor(3114.0334, grad_fn=<SqrtBackward0>)\n",
      "Epoch 60, Training Loss:  tensor(3209.2473, grad_fn=<SqrtBackward0>)\n",
      "Epoch 61, Training Loss:  tensor(3202.4585, grad_fn=<SqrtBackward0>)\n",
      "Epoch 62, Training Loss:  tensor(3112.9473, grad_fn=<SqrtBackward0>)\n",
      "Epoch 63, Training Loss:  tensor(3027.2847, grad_fn=<SqrtBackward0>)\n",
      "Epoch 64, Training Loss:  tensor(3070.2859, grad_fn=<SqrtBackward0>)\n",
      "Epoch 65, Training Loss:  tensor(3046.3398, grad_fn=<SqrtBackward0>)\n",
      "Epoch 66, Training Loss:  tensor(3127.6836, grad_fn=<SqrtBackward0>)\n",
      "Epoch 67, Training Loss:  tensor(3007.6377, grad_fn=<SqrtBackward0>)\n",
      "Epoch 68, Training Loss:  tensor(2952.5752, grad_fn=<SqrtBackward0>)\n",
      "Epoch 69, Training Loss:  tensor(3013.5815, grad_fn=<SqrtBackward0>)\n",
      "Epoch 70, Training Loss:  tensor(2966.7417, grad_fn=<SqrtBackward0>)\n",
      "Epoch 71, Training Loss:  tensor(2924.8259, grad_fn=<SqrtBackward0>)\n",
      "Epoch 72, Training Loss:  tensor(2876.5764, grad_fn=<SqrtBackward0>)\n",
      "Epoch 73, Training Loss:  tensor(2926.9856, grad_fn=<SqrtBackward0>)\n",
      "Epoch 74, Training Loss:  tensor(2713.2341, grad_fn=<SqrtBackward0>)\n",
      "Epoch 75, Training Loss:  tensor(2766.3706, grad_fn=<SqrtBackward0>)\n",
      "Epoch 76, Training Loss:  tensor(2752.8552, grad_fn=<SqrtBackward0>)\n",
      "Epoch 77, Training Loss:  tensor(2710.9241, grad_fn=<SqrtBackward0>)\n",
      "Epoch 78, Training Loss:  tensor(2752.1318, grad_fn=<SqrtBackward0>)\n",
      "Epoch 79, Training Loss:  tensor(2637.7100, grad_fn=<SqrtBackward0>)\n",
      "Epoch 80, Training Loss:  tensor(2580.9680, grad_fn=<SqrtBackward0>)\n",
      "Epoch 81, Training Loss:  tensor(2545.2722, grad_fn=<SqrtBackward0>)\n",
      "Epoch 82, Training Loss:  tensor(2422.8508, grad_fn=<SqrtBackward0>)\n",
      "Epoch 83, Training Loss:  tensor(2480.4417, grad_fn=<SqrtBackward0>)\n",
      "Epoch 84, Training Loss:  tensor(2298.1692, grad_fn=<SqrtBackward0>)\n",
      "Epoch 85, Training Loss:  tensor(2197.3428, grad_fn=<SqrtBackward0>)\n",
      "Epoch 86, Training Loss:  tensor(2105.9043, grad_fn=<SqrtBackward0>)\n",
      "Epoch 87, Training Loss:  tensor(2090.0571, grad_fn=<SqrtBackward0>)\n",
      "Epoch 88, Training Loss:  tensor(2051.5708, grad_fn=<SqrtBackward0>)\n",
      "Epoch 89, Training Loss:  tensor(1956.9697, grad_fn=<SqrtBackward0>)\n",
      "Epoch 90, Training Loss:  tensor(1833.3729, grad_fn=<SqrtBackward0>)\n",
      "Epoch 91, Training Loss:  tensor(1785.8901, grad_fn=<SqrtBackward0>)\n",
      "Epoch 92, Training Loss:  tensor(1898.9727, grad_fn=<SqrtBackward0>)\n",
      "Epoch 93, Training Loss:  tensor(1672.9989, grad_fn=<SqrtBackward0>)\n",
      "Epoch 94, Training Loss:  tensor(1594.8199, grad_fn=<SqrtBackward0>)\n",
      "Epoch 95, Training Loss:  tensor(1520.1964, grad_fn=<SqrtBackward0>)\n",
      "Epoch 96, Training Loss:  tensor(1464.1158, grad_fn=<SqrtBackward0>)\n",
      "Epoch 97, Training Loss:  tensor(1425.5939, grad_fn=<SqrtBackward0>)\n",
      "Epoch 98, Training Loss:  tensor(1363.1569, grad_fn=<SqrtBackward0>)\n",
      "Epoch 99, Training Loss:  tensor(1388.4941, grad_fn=<SqrtBackward0>)\n",
      "Epoch 100, Training Loss:  tensor(1231.9718, grad_fn=<SqrtBackward0>)\n",
      "Epoch 101, Training Loss:  tensor(1092.5383, grad_fn=<SqrtBackward0>)\n",
      "Epoch 102, Training Loss:  tensor(1248.5498, grad_fn=<SqrtBackward0>)\n",
      "Epoch 103, Training Loss:  tensor(1134.4751, grad_fn=<SqrtBackward0>)\n",
      "Epoch 104, Training Loss:  tensor(1037.4519, grad_fn=<SqrtBackward0>)\n",
      "Epoch 105, Training Loss:  tensor(1013.3384, grad_fn=<SqrtBackward0>)\n",
      "Epoch 106, Training Loss:  tensor(930.9197, grad_fn=<SqrtBackward0>)\n",
      "Epoch 107, Training Loss:  tensor(978.7208, grad_fn=<SqrtBackward0>)\n",
      "Epoch 108, Training Loss:  tensor(886.6652, grad_fn=<SqrtBackward0>)\n",
      "Epoch 109, Training Loss:  tensor(863.8636, grad_fn=<SqrtBackward0>)\n",
      "Epoch 110, Training Loss:  tensor(793.9975, grad_fn=<SqrtBackward0>)\n",
      "Epoch 111, Training Loss:  tensor(732.6866, grad_fn=<SqrtBackward0>)\n",
      "Epoch 112, Training Loss:  tensor(763.7050, grad_fn=<SqrtBackward0>)\n",
      "Epoch 113, Training Loss:  tensor(749.2955, grad_fn=<SqrtBackward0>)\n",
      "Epoch 114, Training Loss:  tensor(836.4375, grad_fn=<SqrtBackward0>)\n",
      "Epoch 115, Training Loss:  tensor(638.3945, grad_fn=<SqrtBackward0>)\n",
      "Epoch 116, Training Loss:  tensor(575.5897, grad_fn=<SqrtBackward0>)\n",
      "Epoch 117, Training Loss:  tensor(619.1479, grad_fn=<SqrtBackward0>)\n",
      "Epoch 118, Training Loss:  tensor(554.8172, grad_fn=<SqrtBackward0>)\n",
      "Epoch 119, Training Loss:  tensor(529.1851, grad_fn=<SqrtBackward0>)\n",
      "Epoch 120, Training Loss:  tensor(708.4539, grad_fn=<SqrtBackward0>)\n",
      "Epoch 121, Training Loss:  tensor(745.9772, grad_fn=<SqrtBackward0>)\n",
      "Epoch 122, Training Loss:  tensor(756.4113, grad_fn=<SqrtBackward0>)\n",
      "Epoch 123, Training Loss:  tensor(610.8523, grad_fn=<SqrtBackward0>)\n",
      "Epoch 124, Training Loss:  tensor(600.4030, grad_fn=<SqrtBackward0>)\n",
      "Epoch 125, Training Loss:  tensor(581.3262, grad_fn=<SqrtBackward0>)\n",
      "Epoch 126, Training Loss:  tensor(633.5103, grad_fn=<SqrtBackward0>)\n",
      "Epoch 127, Training Loss:  tensor(593.7592, grad_fn=<SqrtBackward0>)\n",
      "Epoch 128, Training Loss:  tensor(611.5764, grad_fn=<SqrtBackward0>)\n",
      "Epoch 129, Training Loss:  tensor(527.9939, grad_fn=<SqrtBackward0>)\n",
      "Epoch 130, Training Loss:  tensor(449.3749, grad_fn=<SqrtBackward0>)\n",
      "Epoch 131, Training Loss:  tensor(490.8782, grad_fn=<SqrtBackward0>)\n",
      "Epoch 132, Training Loss:  tensor(564.0975, grad_fn=<SqrtBackward0>)\n",
      "Epoch 133, Training Loss:  tensor(424.0433, grad_fn=<SqrtBackward0>)\n",
      "Epoch 134, Training Loss:  tensor(436.2055, grad_fn=<SqrtBackward0>)\n",
      "Epoch 135, Training Loss:  tensor(453.6778, grad_fn=<SqrtBackward0>)\n",
      "Epoch 136, Training Loss:  tensor(449.0599, grad_fn=<SqrtBackward0>)\n",
      "Epoch 137, Training Loss:  tensor(474.8511, grad_fn=<SqrtBackward0>)\n",
      "Epoch 138, Training Loss:  tensor(334.5663, grad_fn=<SqrtBackward0>)\n",
      "Epoch 139, Training Loss:  tensor(372.2966, grad_fn=<SqrtBackward0>)\n",
      "Epoch 140, Training Loss:  tensor(378.8000, grad_fn=<SqrtBackward0>)\n",
      "Epoch 141, Training Loss:  tensor(357.2718, grad_fn=<SqrtBackward0>)\n",
      "Epoch 142, Training Loss:  tensor(350.3521, grad_fn=<SqrtBackward0>)\n",
      "Epoch 143, Training Loss:  tensor(353.2730, grad_fn=<SqrtBackward0>)\n",
      "Epoch 144, Training Loss:  tensor(353.2941, grad_fn=<SqrtBackward0>)\n",
      "Epoch 145, Training Loss:  tensor(363.4376, grad_fn=<SqrtBackward0>)\n",
      "Epoch 146, Training Loss:  tensor(312.3448, grad_fn=<SqrtBackward0>)\n",
      "Epoch 147, Training Loss:  tensor(297.1118, grad_fn=<SqrtBackward0>)\n",
      "Epoch 148, Training Loss:  tensor(306.2867, grad_fn=<SqrtBackward0>)\n",
      "Epoch 149, Training Loss:  tensor(297.1279, grad_fn=<SqrtBackward0>)\n",
      "Epoch 150, Training Loss:  tensor(266.7033, grad_fn=<SqrtBackward0>)\n",
      "Epoch 151, Training Loss:  tensor(318.2951, grad_fn=<SqrtBackward0>)\n",
      "Epoch 152, Training Loss:  tensor(347.9288, grad_fn=<SqrtBackward0>)\n",
      "Epoch 153, Training Loss:  tensor(358.2667, grad_fn=<SqrtBackward0>)\n",
      "Epoch 154, Training Loss:  tensor(290.3937, grad_fn=<SqrtBackward0>)\n",
      "Epoch 155, Training Loss:  tensor(245.5008, grad_fn=<SqrtBackward0>)\n",
      "Epoch 156, Training Loss:  tensor(262.7671, grad_fn=<SqrtBackward0>)\n",
      "Epoch 157, Training Loss:  tensor(240.2182, grad_fn=<SqrtBackward0>)\n",
      "Epoch 158, Training Loss:  tensor(278.0908, grad_fn=<SqrtBackward0>)\n",
      "Epoch 159, Training Loss:  tensor(357.2216, grad_fn=<SqrtBackward0>)\n",
      "Epoch 160, Training Loss:  tensor(262.8315, grad_fn=<SqrtBackward0>)\n",
      "Epoch 161, Training Loss:  tensor(475.2706, grad_fn=<SqrtBackward0>)\n",
      "Epoch 162, Training Loss:  tensor(354.6844, grad_fn=<SqrtBackward0>)\n",
      "Epoch 163, Training Loss:  tensor(296.0417, grad_fn=<SqrtBackward0>)\n",
      "Epoch 164, Training Loss:  tensor(390.0928, grad_fn=<SqrtBackward0>)\n",
      "Epoch 165, Training Loss:  tensor(362.3904, grad_fn=<SqrtBackward0>)\n",
      "Epoch 166, Training Loss:  tensor(268.2266, grad_fn=<SqrtBackward0>)\n",
      "Epoch 167, Training Loss:  tensor(259.1939, grad_fn=<SqrtBackward0>)\n",
      "Epoch 168, Training Loss:  tensor(295.3192, grad_fn=<SqrtBackward0>)\n",
      "Epoch 169, Training Loss:  tensor(367.7832, grad_fn=<SqrtBackward0>)\n",
      "Epoch 170, Training Loss:  tensor(348.7807, grad_fn=<SqrtBackward0>)\n",
      "Epoch 171, Training Loss:  tensor(431.3298, grad_fn=<SqrtBackward0>)\n",
      "Epoch 172, Training Loss:  tensor(359.3923, grad_fn=<SqrtBackward0>)\n",
      "Epoch 173, Training Loss:  tensor(330.3672, grad_fn=<SqrtBackward0>)\n",
      "Epoch 174, Training Loss:  tensor(335.7946, grad_fn=<SqrtBackward0>)\n",
      "Epoch 175, Training Loss:  tensor(285.8520, grad_fn=<SqrtBackward0>)\n",
      "Epoch 176, Training Loss:  tensor(282.1229, grad_fn=<SqrtBackward0>)\n",
      "Epoch 177, Training Loss:  tensor(254.0238, grad_fn=<SqrtBackward0>)\n",
      "Epoch 178, Training Loss:  tensor(234.4465, grad_fn=<SqrtBackward0>)\n",
      "Epoch 179, Training Loss:  tensor(373.2974, grad_fn=<SqrtBackward0>)\n",
      "Epoch 180, Training Loss:  tensor(335.8456, grad_fn=<SqrtBackward0>)\n",
      "Epoch 181, Training Loss:  tensor(253.4694, grad_fn=<SqrtBackward0>)\n",
      "Epoch 182, Training Loss:  tensor(252.8332, grad_fn=<SqrtBackward0>)\n",
      "Epoch 183, Training Loss:  tensor(236.5259, grad_fn=<SqrtBackward0>)\n",
      "Epoch 184, Training Loss:  tensor(282.0636, grad_fn=<SqrtBackward0>)\n",
      "Epoch 185, Training Loss:  tensor(261.4648, grad_fn=<SqrtBackward0>)\n",
      "Epoch 186, Training Loss:  tensor(280.6161, grad_fn=<SqrtBackward0>)\n",
      "Epoch 187, Training Loss:  tensor(260.9843, grad_fn=<SqrtBackward0>)\n",
      "Epoch 188, Training Loss:  tensor(236.6052, grad_fn=<SqrtBackward0>)\n",
      "Epoch 189, Training Loss:  tensor(265.2905, grad_fn=<SqrtBackward0>)\n",
      "Epoch 190, Training Loss:  tensor(248.4677, grad_fn=<SqrtBackward0>)\n",
      "Epoch 191, Training Loss:  tensor(272.0227, grad_fn=<SqrtBackward0>)\n",
      "Epoch 192, Training Loss:  tensor(298.3594, grad_fn=<SqrtBackward0>)\n",
      "Epoch 193, Training Loss:  tensor(287.5892, grad_fn=<SqrtBackward0>)\n",
      "Epoch 194, Training Loss:  tensor(269.9214, grad_fn=<SqrtBackward0>)\n",
      "Epoch 195, Training Loss:  tensor(277.3510, grad_fn=<SqrtBackward0>)\n",
      "Epoch 196, Training Loss:  tensor(217.5208, grad_fn=<SqrtBackward0>)\n",
      "Epoch 197, Training Loss:  tensor(274.5240, grad_fn=<SqrtBackward0>)\n",
      "Epoch 198, Training Loss:  tensor(245.5418, grad_fn=<SqrtBackward0>)\n",
      "Epoch 199, Training Loss:  tensor(277.2258, grad_fn=<SqrtBackward0>)\n",
      "Epoch 200, Training Loss:  tensor(253.8495, grad_fn=<SqrtBackward0>)\n",
      "Epoch 201, Training Loss:  tensor(289.8831, grad_fn=<SqrtBackward0>)\n",
      "Epoch 202, Training Loss:  tensor(233.3112, grad_fn=<SqrtBackward0>)\n",
      "Epoch 203, Training Loss:  tensor(256.8823, grad_fn=<SqrtBackward0>)\n",
      "Epoch 204, Training Loss:  tensor(255.2471, grad_fn=<SqrtBackward0>)\n",
      "Epoch 205, Training Loss:  tensor(246.3004, grad_fn=<SqrtBackward0>)\n",
      "Epoch 206, Training Loss:  tensor(259.1100, grad_fn=<SqrtBackward0>)\n",
      "Epoch 207, Training Loss:  tensor(252.5328, grad_fn=<SqrtBackward0>)\n",
      "Epoch 208, Training Loss:  tensor(278.2173, grad_fn=<SqrtBackward0>)\n",
      "Epoch 209, Training Loss:  tensor(249.0822, grad_fn=<SqrtBackward0>)\n",
      "Epoch 210, Training Loss:  tensor(195.5066, grad_fn=<SqrtBackward0>)\n",
      "Epoch 211, Training Loss:  tensor(314.1277, grad_fn=<SqrtBackward0>)\n",
      "Epoch 212, Training Loss:  tensor(232.1695, grad_fn=<SqrtBackward0>)\n",
      "Epoch 213, Training Loss:  tensor(204.7855, grad_fn=<SqrtBackward0>)\n",
      "Epoch 214, Training Loss:  tensor(221.7229, grad_fn=<SqrtBackward0>)\n",
      "Epoch 215, Training Loss:  tensor(254.1413, grad_fn=<SqrtBackward0>)\n",
      "Epoch 216, Training Loss:  tensor(198.9998, grad_fn=<SqrtBackward0>)\n",
      "Epoch 217, Training Loss:  tensor(233.7847, grad_fn=<SqrtBackward0>)\n",
      "Epoch 218, Training Loss:  tensor(197.7475, grad_fn=<SqrtBackward0>)\n",
      "Epoch 219, Training Loss:  tensor(271.7844, grad_fn=<SqrtBackward0>)\n",
      "Epoch 220, Training Loss:  tensor(180.9235, grad_fn=<SqrtBackward0>)\n",
      "Epoch 221, Training Loss:  tensor(200.3573, grad_fn=<SqrtBackward0>)\n",
      "Epoch 222, Training Loss:  tensor(239.7713, grad_fn=<SqrtBackward0>)\n",
      "Epoch 223, Training Loss:  tensor(275.5910, grad_fn=<SqrtBackward0>)\n",
      "Epoch 224, Training Loss:  tensor(250.2030, grad_fn=<SqrtBackward0>)\n",
      "Epoch 225, Training Loss:  tensor(363.5391, grad_fn=<SqrtBackward0>)\n",
      "Epoch 226, Training Loss:  tensor(155.1805, grad_fn=<SqrtBackward0>)\n",
      "Epoch 227, Training Loss:  tensor(144.0182, grad_fn=<SqrtBackward0>)\n",
      "Epoch 228, Training Loss:  tensor(127.2132, grad_fn=<SqrtBackward0>)\n",
      "Epoch 229, Training Loss:  tensor(144.3247, grad_fn=<SqrtBackward0>)\n",
      "Epoch 230, Training Loss:  tensor(144.6068, grad_fn=<SqrtBackward0>)\n",
      "Epoch 231, Training Loss:  tensor(118.3327, grad_fn=<SqrtBackward0>)\n",
      "Epoch 232, Training Loss:  tensor(158.2788, grad_fn=<SqrtBackward0>)\n",
      "Epoch 233, Training Loss:  tensor(139.5444, grad_fn=<SqrtBackward0>)\n",
      "Epoch 234, Training Loss:  tensor(130.9488, grad_fn=<SqrtBackward0>)\n",
      "Epoch 235, Training Loss:  tensor(134.8279, grad_fn=<SqrtBackward0>)\n",
      "Epoch 236, Training Loss:  tensor(165.2841, grad_fn=<SqrtBackward0>)\n",
      "Epoch 237, Training Loss:  tensor(209.3302, grad_fn=<SqrtBackward0>)\n",
      "Epoch 238, Training Loss:  tensor(162.9387, grad_fn=<SqrtBackward0>)\n",
      "Epoch 239, Training Loss:  tensor(149.8273, grad_fn=<SqrtBackward0>)\n",
      "Epoch 240, Training Loss:  tensor(207.6810, grad_fn=<SqrtBackward0>)\n",
      "Epoch 241, Training Loss:  tensor(266.2141, grad_fn=<SqrtBackward0>)\n",
      "Epoch 242, Training Loss:  tensor(294.2834, grad_fn=<SqrtBackward0>)\n",
      "Epoch 243, Training Loss:  tensor(297.3734, grad_fn=<SqrtBackward0>)\n",
      "Epoch 244, Training Loss:  tensor(327.6888, grad_fn=<SqrtBackward0>)\n",
      "Epoch 245, Training Loss:  tensor(183.8829, grad_fn=<SqrtBackward0>)\n",
      "Epoch 246, Training Loss:  tensor(208.3633, grad_fn=<SqrtBackward0>)\n",
      "Epoch 247, Training Loss:  tensor(296.8228, grad_fn=<SqrtBackward0>)\n",
      "Epoch 248, Training Loss:  tensor(308.7157, grad_fn=<SqrtBackward0>)\n",
      "Epoch 249, Training Loss:  tensor(138.0370, grad_fn=<SqrtBackward0>)\n",
      "Epoch 250, Training Loss:  tensor(233.4938, grad_fn=<SqrtBackward0>)\n",
      "Epoch 251, Training Loss:  tensor(161.9025, grad_fn=<SqrtBackward0>)\n",
      "Epoch 252, Training Loss:  tensor(333.4238, grad_fn=<SqrtBackward0>)\n",
      "Epoch 253, Training Loss:  tensor(199.0551, grad_fn=<SqrtBackward0>)\n",
      "Epoch 254, Training Loss:  tensor(208.8862, grad_fn=<SqrtBackward0>)\n",
      "Epoch 255, Training Loss:  tensor(255.6454, grad_fn=<SqrtBackward0>)\n",
      "Epoch 256, Training Loss:  tensor(246.4230, grad_fn=<SqrtBackward0>)\n",
      "Epoch 257, Training Loss:  tensor(190.0796, grad_fn=<SqrtBackward0>)\n",
      "Epoch 258, Training Loss:  tensor(210.5685, grad_fn=<SqrtBackward0>)\n",
      "Epoch 259, Training Loss:  tensor(222.3389, grad_fn=<SqrtBackward0>)\n",
      "Epoch 260, Training Loss:  tensor(234.2664, grad_fn=<SqrtBackward0>)\n",
      "Epoch 261, Training Loss:  tensor(252.8101, grad_fn=<SqrtBackward0>)\n",
      "Epoch 262, Training Loss:  tensor(223.2910, grad_fn=<SqrtBackward0>)\n",
      "Epoch 263, Training Loss:  tensor(186.6844, grad_fn=<SqrtBackward0>)\n",
      "Epoch 264, Training Loss:  tensor(237.3692, grad_fn=<SqrtBackward0>)\n",
      "Epoch 265, Training Loss:  tensor(267.7550, grad_fn=<SqrtBackward0>)\n",
      "Epoch 266, Training Loss:  tensor(245.1329, grad_fn=<SqrtBackward0>)\n",
      "Epoch 267, Training Loss:  tensor(234.8142, grad_fn=<SqrtBackward0>)\n",
      "Epoch 268, Training Loss:  tensor(288.3876, grad_fn=<SqrtBackward0>)\n",
      "Epoch 269, Training Loss:  tensor(212.0768, grad_fn=<SqrtBackward0>)\n",
      "Epoch 270, Training Loss:  tensor(244.2312, grad_fn=<SqrtBackward0>)\n",
      "Epoch 271, Training Loss:  tensor(234.5365, grad_fn=<SqrtBackward0>)\n",
      "Epoch 272, Training Loss:  tensor(212.9260, grad_fn=<SqrtBackward0>)\n",
      "Epoch 273, Training Loss:  tensor(241.7040, grad_fn=<SqrtBackward0>)\n",
      "Epoch 274, Training Loss:  tensor(238.6768, grad_fn=<SqrtBackward0>)\n",
      "Epoch 275, Training Loss:  tensor(166.7026, grad_fn=<SqrtBackward0>)\n",
      "Epoch 276, Training Loss:  tensor(245.2706, grad_fn=<SqrtBackward0>)\n",
      "Epoch 277, Training Loss:  tensor(192.8139, grad_fn=<SqrtBackward0>)\n",
      "Epoch 278, Training Loss:  tensor(215.1930, grad_fn=<SqrtBackward0>)\n",
      "Epoch 279, Training Loss:  tensor(175.6848, grad_fn=<SqrtBackward0>)\n",
      "Epoch 280, Training Loss:  tensor(230.9589, grad_fn=<SqrtBackward0>)\n",
      "Epoch 281, Training Loss:  tensor(208.7213, grad_fn=<SqrtBackward0>)\n",
      "Epoch 282, Training Loss:  tensor(302.8200, grad_fn=<SqrtBackward0>)\n",
      "Epoch 283, Training Loss:  tensor(191.9758, grad_fn=<SqrtBackward0>)\n",
      "Epoch 284, Training Loss:  tensor(237.5422, grad_fn=<SqrtBackward0>)\n",
      "Epoch 285, Training Loss:  tensor(242.6047, grad_fn=<SqrtBackward0>)\n",
      "Epoch 286, Training Loss:  tensor(197.3140, grad_fn=<SqrtBackward0>)\n",
      "Epoch 287, Training Loss:  tensor(262.5711, grad_fn=<SqrtBackward0>)\n",
      "Epoch 288, Training Loss:  tensor(206.4036, grad_fn=<SqrtBackward0>)\n",
      "Epoch 289, Training Loss:  tensor(251.7814, grad_fn=<SqrtBackward0>)\n",
      "Epoch 290, Training Loss:  tensor(238.6753, grad_fn=<SqrtBackward0>)\n",
      "Epoch 291, Training Loss:  tensor(220.1642, grad_fn=<SqrtBackward0>)\n",
      "Epoch 292, Training Loss:  tensor(245.2164, grad_fn=<SqrtBackward0>)\n",
      "Epoch 293, Training Loss:  tensor(136.5984, grad_fn=<SqrtBackward0>)\n",
      "Epoch 294, Training Loss:  tensor(207.1643, grad_fn=<SqrtBackward0>)\n",
      "Epoch 295, Training Loss:  tensor(182.2391, grad_fn=<SqrtBackward0>)\n",
      "Epoch 296, Training Loss:  tensor(231.4779, grad_fn=<SqrtBackward0>)\n",
      "Epoch 297, Training Loss:  tensor(238.7756, grad_fn=<SqrtBackward0>)\n",
      "Epoch 298, Training Loss:  tensor(234.6846, grad_fn=<SqrtBackward0>)\n",
      "Epoch 299, Training Loss:  tensor(165.0103, grad_fn=<SqrtBackward0>)\n",
      "Epoch 300, Training Loss:  tensor(223.9195, grad_fn=<SqrtBackward0>)\n",
      "Epoch 301, Training Loss:  tensor(222.6721, grad_fn=<SqrtBackward0>)\n",
      "Epoch 302, Training Loss:  tensor(204.8369, grad_fn=<SqrtBackward0>)\n",
      "Epoch 303, Training Loss:  tensor(221.9419, grad_fn=<SqrtBackward0>)\n",
      "Epoch 304, Training Loss:  tensor(218.3433, grad_fn=<SqrtBackward0>)\n",
      "Epoch 305, Training Loss:  tensor(236.8763, grad_fn=<SqrtBackward0>)\n",
      "Epoch 306, Training Loss:  tensor(205.0567, grad_fn=<SqrtBackward0>)\n",
      "Epoch 307, Training Loss:  tensor(229.3513, grad_fn=<SqrtBackward0>)\n",
      "Epoch 308, Training Loss:  tensor(189.6441, grad_fn=<SqrtBackward0>)\n",
      "Epoch 309, Training Loss:  tensor(238.1910, grad_fn=<SqrtBackward0>)\n",
      "Epoch 310, Training Loss:  tensor(221.8081, grad_fn=<SqrtBackward0>)\n",
      "Epoch 311, Training Loss:  tensor(183.3784, grad_fn=<SqrtBackward0>)\n",
      "Epoch 312, Training Loss:  tensor(153.1503, grad_fn=<SqrtBackward0>)\n",
      "Epoch 313, Training Loss:  tensor(203.4444, grad_fn=<SqrtBackward0>)\n",
      "Epoch 314, Training Loss:  tensor(218.4081, grad_fn=<SqrtBackward0>)\n",
      "Epoch 315, Training Loss:  tensor(172.1590, grad_fn=<SqrtBackward0>)\n",
      "Epoch 316, Training Loss:  tensor(238.6058, grad_fn=<SqrtBackward0>)\n",
      "Epoch 317, Training Loss:  tensor(186.1551, grad_fn=<SqrtBackward0>)\n",
      "Epoch 318, Training Loss:  tensor(244.7153, grad_fn=<SqrtBackward0>)\n",
      "Epoch 319, Training Loss:  tensor(220.9061, grad_fn=<SqrtBackward0>)\n",
      "Epoch 320, Training Loss:  tensor(219.3928, grad_fn=<SqrtBackward0>)\n",
      "Epoch 321, Training Loss:  tensor(243.2809, grad_fn=<SqrtBackward0>)\n",
      "Epoch 322, Training Loss:  tensor(221.0590, grad_fn=<SqrtBackward0>)\n",
      "Epoch 323, Training Loss:  tensor(204.4927, grad_fn=<SqrtBackward0>)\n",
      "Epoch 324, Training Loss:  tensor(219.8716, grad_fn=<SqrtBackward0>)\n",
      "Epoch 325, Training Loss:  tensor(232.7359, grad_fn=<SqrtBackward0>)\n",
      "Epoch 326, Training Loss:  tensor(188.8441, grad_fn=<SqrtBackward0>)\n",
      "Epoch 327, Training Loss:  tensor(170.0151, grad_fn=<SqrtBackward0>)\n",
      "Epoch 328, Training Loss:  tensor(217.4731, grad_fn=<SqrtBackward0>)\n",
      "Epoch 329, Training Loss:  tensor(241.2389, grad_fn=<SqrtBackward0>)\n",
      "Epoch 330, Training Loss:  tensor(136.4203, grad_fn=<SqrtBackward0>)\n",
      "Epoch 331, Training Loss:  tensor(235.7790, grad_fn=<SqrtBackward0>)\n",
      "Epoch 332, Training Loss:  tensor(173.5070, grad_fn=<SqrtBackward0>)\n",
      "Epoch 333, Training Loss:  tensor(245.2740, grad_fn=<SqrtBackward0>)\n",
      "Epoch 334, Training Loss:  tensor(207.0491, grad_fn=<SqrtBackward0>)\n",
      "Epoch 335, Training Loss:  tensor(242.0945, grad_fn=<SqrtBackward0>)\n",
      "Epoch 336, Training Loss:  tensor(211.8703, grad_fn=<SqrtBackward0>)\n",
      "Epoch 337, Training Loss:  tensor(175.6516, grad_fn=<SqrtBackward0>)\n",
      "Epoch 338, Training Loss:  tensor(207.9702, grad_fn=<SqrtBackward0>)\n",
      "Epoch 339, Training Loss:  tensor(166.7120, grad_fn=<SqrtBackward0>)\n",
      "Epoch 340, Training Loss:  tensor(131.8772, grad_fn=<SqrtBackward0>)\n",
      "Epoch 341, Training Loss:  tensor(214.7368, grad_fn=<SqrtBackward0>)\n",
      "Epoch 342, Training Loss:  tensor(294.4765, grad_fn=<SqrtBackward0>)\n",
      "Epoch 343, Training Loss:  tensor(76.5365, grad_fn=<SqrtBackward0>)\n",
      "Epoch 344, Training Loss:  tensor(96.5845, grad_fn=<SqrtBackward0>)\n",
      "Epoch 345, Training Loss:  tensor(138.1578, grad_fn=<SqrtBackward0>)\n",
      "Epoch 346, Training Loss:  tensor(119.6619, grad_fn=<SqrtBackward0>)\n",
      "Epoch 347, Training Loss:  tensor(79.6028, grad_fn=<SqrtBackward0>)\n",
      "Epoch 348, Training Loss:  tensor(122.9213, grad_fn=<SqrtBackward0>)\n",
      "Epoch 349, Training Loss:  tensor(166.4445, grad_fn=<SqrtBackward0>)\n",
      "Epoch 350, Training Loss:  tensor(81.7109, grad_fn=<SqrtBackward0>)\n",
      "Epoch 351, Training Loss:  tensor(167.6765, grad_fn=<SqrtBackward0>)\n",
      "Epoch 352, Training Loss:  tensor(129.6903, grad_fn=<SqrtBackward0>)\n",
      "Epoch 353, Training Loss:  tensor(183.5470, grad_fn=<SqrtBackward0>)\n",
      "Epoch 354, Training Loss:  tensor(146.3577, grad_fn=<SqrtBackward0>)\n",
      "Epoch 355, Training Loss:  tensor(145.0764, grad_fn=<SqrtBackward0>)\n",
      "Epoch 356, Training Loss:  tensor(111.5229, grad_fn=<SqrtBackward0>)\n",
      "Epoch 357, Training Loss:  tensor(123.7796, grad_fn=<SqrtBackward0>)\n",
      "Epoch 358, Training Loss:  tensor(153.4668, grad_fn=<SqrtBackward0>)\n",
      "Epoch 359, Training Loss:  tensor(167.3084, grad_fn=<SqrtBackward0>)\n",
      "Epoch 360, Training Loss:  tensor(161.2533, grad_fn=<SqrtBackward0>)\n",
      "Epoch 361, Training Loss:  tensor(157.6076, grad_fn=<SqrtBackward0>)\n",
      "Epoch 362, Training Loss:  tensor(262.9078, grad_fn=<SqrtBackward0>)\n",
      "Epoch 363, Training Loss:  tensor(123.1088, grad_fn=<SqrtBackward0>)\n",
      "Epoch 364, Training Loss:  tensor(142.2796, grad_fn=<SqrtBackward0>)\n",
      "Epoch 365, Training Loss:  tensor(124.3003, grad_fn=<SqrtBackward0>)\n",
      "Epoch 366, Training Loss:  tensor(486.6093, grad_fn=<SqrtBackward0>)\n",
      "Epoch 367, Training Loss:  tensor(163.5902, grad_fn=<SqrtBackward0>)\n",
      "Epoch 368, Training Loss:  tensor(237.4129, grad_fn=<SqrtBackward0>)\n",
      "Epoch 369, Training Loss:  tensor(105.2077, grad_fn=<SqrtBackward0>)\n",
      "Epoch 370, Training Loss:  tensor(236.4418, grad_fn=<SqrtBackward0>)\n",
      "Epoch 371, Training Loss:  tensor(141.0438, grad_fn=<SqrtBackward0>)\n",
      "Epoch 372, Training Loss:  tensor(80.0276, grad_fn=<SqrtBackward0>)\n",
      "Epoch 373, Training Loss:  tensor(102.2989, grad_fn=<SqrtBackward0>)\n",
      "Epoch 374, Training Loss:  tensor(84.8607, grad_fn=<SqrtBackward0>)\n",
      "Epoch 375, Training Loss:  tensor(84.8820, grad_fn=<SqrtBackward0>)\n",
      "Epoch 376, Training Loss:  tensor(66.4742, grad_fn=<SqrtBackward0>)\n",
      "Epoch 377, Training Loss:  tensor(120.0087, grad_fn=<SqrtBackward0>)\n",
      "Epoch 378, Training Loss:  tensor(105.9312, grad_fn=<SqrtBackward0>)\n",
      "Epoch 379, Training Loss:  tensor(88.0048, grad_fn=<SqrtBackward0>)\n",
      "Epoch 380, Training Loss:  tensor(92.1156, grad_fn=<SqrtBackward0>)\n",
      "Epoch 381, Training Loss:  tensor(85.3403, grad_fn=<SqrtBackward0>)\n",
      "Epoch 382, Training Loss:  tensor(78.1618, grad_fn=<SqrtBackward0>)\n",
      "Epoch 383, Training Loss:  tensor(117.1426, grad_fn=<SqrtBackward0>)\n",
      "Epoch 384, Training Loss:  tensor(105.3797, grad_fn=<SqrtBackward0>)\n",
      "Epoch 385, Training Loss:  tensor(122.7325, grad_fn=<SqrtBackward0>)\n",
      "Epoch 386, Training Loss:  tensor(144.6111, grad_fn=<SqrtBackward0>)\n",
      "Epoch 387, Training Loss:  tensor(124.5026, grad_fn=<SqrtBackward0>)\n",
      "Epoch 388, Training Loss:  tensor(118.8429, grad_fn=<SqrtBackward0>)\n",
      "Epoch 389, Training Loss:  tensor(95.0137, grad_fn=<SqrtBackward0>)\n",
      "Epoch 390, Training Loss:  tensor(197.6779, grad_fn=<SqrtBackward0>)\n",
      "Epoch 391, Training Loss:  tensor(218.7159, grad_fn=<SqrtBackward0>)\n",
      "Epoch 392, Training Loss:  tensor(145.7498, grad_fn=<SqrtBackward0>)\n",
      "Epoch 393, Training Loss:  tensor(278.4532, grad_fn=<SqrtBackward0>)\n",
      "Epoch 394, Training Loss:  tensor(185.3214, grad_fn=<SqrtBackward0>)\n",
      "Epoch 395, Training Loss:  tensor(211.0975, grad_fn=<SqrtBackward0>)\n",
      "Epoch 396, Training Loss:  tensor(292.3935, grad_fn=<SqrtBackward0>)\n",
      "Epoch 397, Training Loss:  tensor(235.8930, grad_fn=<SqrtBackward0>)\n",
      "Epoch 398, Training Loss:  tensor(188.8104, grad_fn=<SqrtBackward0>)\n",
      "Epoch 399, Training Loss:  tensor(259.3536, grad_fn=<SqrtBackward0>)\n",
      "Epoch 400, Training Loss:  tensor(183.4101, grad_fn=<SqrtBackward0>)\n",
      "Epoch 401, Training Loss:  tensor(178.8247, grad_fn=<SqrtBackward0>)\n",
      "Epoch 402, Training Loss:  tensor(283.8645, grad_fn=<SqrtBackward0>)\n",
      "Epoch 403, Training Loss:  tensor(172.0551, grad_fn=<SqrtBackward0>)\n",
      "Epoch 404, Training Loss:  tensor(140.2389, grad_fn=<SqrtBackward0>)\n",
      "Epoch 405, Training Loss:  tensor(239.6915, grad_fn=<SqrtBackward0>)\n",
      "Epoch 406, Training Loss:  tensor(135.5684, grad_fn=<SqrtBackward0>)\n",
      "Epoch 407, Training Loss:  tensor(288.8033, grad_fn=<SqrtBackward0>)\n",
      "Epoch 408, Training Loss:  tensor(145.8033, grad_fn=<SqrtBackward0>)\n",
      "Epoch 409, Training Loss:  tensor(173.7406, grad_fn=<SqrtBackward0>)\n",
      "Epoch 410, Training Loss:  tensor(80.4454, grad_fn=<SqrtBackward0>)\n",
      "Epoch 411, Training Loss:  tensor(194.2164, grad_fn=<SqrtBackward0>)\n",
      "Epoch 412, Training Loss:  tensor(166.5581, grad_fn=<SqrtBackward0>)\n",
      "Epoch 413, Training Loss:  tensor(201.8156, grad_fn=<SqrtBackward0>)\n",
      "Epoch 414, Training Loss:  tensor(227.4571, grad_fn=<SqrtBackward0>)\n",
      "Epoch 415, Training Loss:  tensor(230.3617, grad_fn=<SqrtBackward0>)\n",
      "Epoch 416, Training Loss:  tensor(118.2625, grad_fn=<SqrtBackward0>)\n",
      "Epoch 417, Training Loss:  tensor(259.4800, grad_fn=<SqrtBackward0>)\n",
      "Epoch 418, Training Loss:  tensor(130.6622, grad_fn=<SqrtBackward0>)\n",
      "Epoch 419, Training Loss:  tensor(181.5865, grad_fn=<SqrtBackward0>)\n",
      "Epoch 420, Training Loss:  tensor(184.6800, grad_fn=<SqrtBackward0>)\n",
      "Epoch 421, Training Loss:  tensor(184.0300, grad_fn=<SqrtBackward0>)\n",
      "Epoch 422, Training Loss:  tensor(224.9550, grad_fn=<SqrtBackward0>)\n",
      "Epoch 423, Training Loss:  tensor(208.9853, grad_fn=<SqrtBackward0>)\n",
      "Epoch 424, Training Loss:  tensor(177.0314, grad_fn=<SqrtBackward0>)\n",
      "Epoch 425, Training Loss:  tensor(211.1341, grad_fn=<SqrtBackward0>)\n",
      "Epoch 426, Training Loss:  tensor(198.5115, grad_fn=<SqrtBackward0>)\n",
      "Epoch 427, Training Loss:  tensor(245.3309, grad_fn=<SqrtBackward0>)\n",
      "Epoch 428, Training Loss:  tensor(122.1476, grad_fn=<SqrtBackward0>)\n",
      "Epoch 429, Training Loss:  tensor(245.5229, grad_fn=<SqrtBackward0>)\n",
      "Epoch 430, Training Loss:  tensor(195.2092, grad_fn=<SqrtBackward0>)\n",
      "Epoch 431, Training Loss:  tensor(212.2363, grad_fn=<SqrtBackward0>)\n",
      "Epoch 432, Training Loss:  tensor(168.2527, grad_fn=<SqrtBackward0>)\n",
      "Epoch 433, Training Loss:  tensor(200.0263, grad_fn=<SqrtBackward0>)\n",
      "Epoch 434, Training Loss:  tensor(91.8472, grad_fn=<SqrtBackward0>)\n",
      "Epoch 435, Training Loss:  tensor(261.4028, grad_fn=<SqrtBackward0>)\n",
      "Epoch 436, Training Loss:  tensor(146.1912, grad_fn=<SqrtBackward0>)\n",
      "Epoch 437, Training Loss:  tensor(240.0999, grad_fn=<SqrtBackward0>)\n",
      "Epoch 438, Training Loss:  tensor(273.7717, grad_fn=<SqrtBackward0>)\n",
      "Epoch 439, Training Loss:  tensor(198.0167, grad_fn=<SqrtBackward0>)\n",
      "Epoch 440, Training Loss:  tensor(264.8562, grad_fn=<SqrtBackward0>)\n",
      "Epoch 441, Training Loss:  tensor(170.0551, grad_fn=<SqrtBackward0>)\n",
      "Epoch 442, Training Loss:  tensor(164.7814, grad_fn=<SqrtBackward0>)\n",
      "Epoch 443, Training Loss:  tensor(102.4769, grad_fn=<SqrtBackward0>)\n",
      "Epoch 444, Training Loss:  tensor(191.7859, grad_fn=<SqrtBackward0>)\n",
      "Epoch 445, Training Loss:  tensor(153.4573, grad_fn=<SqrtBackward0>)\n",
      "Epoch 446, Training Loss:  tensor(203.2889, grad_fn=<SqrtBackward0>)\n",
      "Epoch 447, Training Loss:  tensor(146.3938, grad_fn=<SqrtBackward0>)\n",
      "Epoch 448, Training Loss:  tensor(198.6762, grad_fn=<SqrtBackward0>)\n",
      "Epoch 449, Training Loss:  tensor(146.0662, grad_fn=<SqrtBackward0>)\n",
      "Epoch 450, Training Loss:  tensor(167.2946, grad_fn=<SqrtBackward0>)\n",
      "Epoch 451, Training Loss:  tensor(112.1636, grad_fn=<SqrtBackward0>)\n",
      "Epoch 452, Training Loss:  tensor(209.4749, grad_fn=<SqrtBackward0>)\n",
      "Epoch 453, Training Loss:  tensor(114.4356, grad_fn=<SqrtBackward0>)\n",
      "Epoch 454, Training Loss:  tensor(291.6390, grad_fn=<SqrtBackward0>)\n",
      "Epoch 455, Training Loss:  tensor(116.5706, grad_fn=<SqrtBackward0>)\n",
      "Epoch 456, Training Loss:  tensor(154.7731, grad_fn=<SqrtBackward0>)\n",
      "Epoch 457, Training Loss:  tensor(121.9849, grad_fn=<SqrtBackward0>)\n",
      "Epoch 458, Training Loss:  tensor(264.4419, grad_fn=<SqrtBackward0>)\n",
      "Epoch 459, Training Loss:  tensor(137.1830, grad_fn=<SqrtBackward0>)\n",
      "Epoch 460, Training Loss:  tensor(197.9314, grad_fn=<SqrtBackward0>)\n",
      "Epoch 461, Training Loss:  tensor(141.0376, grad_fn=<SqrtBackward0>)\n",
      "Epoch 462, Training Loss:  tensor(142.7331, grad_fn=<SqrtBackward0>)\n",
      "Epoch 463, Training Loss:  tensor(250.3692, grad_fn=<SqrtBackward0>)\n",
      "Epoch 464, Training Loss:  tensor(146.6700, grad_fn=<SqrtBackward0>)\n",
      "Epoch 465, Training Loss:  tensor(174.0193, grad_fn=<SqrtBackward0>)\n",
      "Epoch 466, Training Loss:  tensor(228.2433, grad_fn=<SqrtBackward0>)\n",
      "Epoch 467, Training Loss:  tensor(127.5337, grad_fn=<SqrtBackward0>)\n",
      "Epoch 468, Training Loss:  tensor(168.7201, grad_fn=<SqrtBackward0>)\n",
      "Epoch 469, Training Loss:  tensor(251.5740, grad_fn=<SqrtBackward0>)\n",
      "Epoch 470, Training Loss:  tensor(146.1971, grad_fn=<SqrtBackward0>)\n",
      "Epoch 471, Training Loss:  tensor(248.6154, grad_fn=<SqrtBackward0>)\n",
      "Epoch 472, Training Loss:  tensor(125.6182, grad_fn=<SqrtBackward0>)\n",
      "Epoch 473, Training Loss:  tensor(242.5147, grad_fn=<SqrtBackward0>)\n",
      "Epoch 474, Training Loss:  tensor(203.2039, grad_fn=<SqrtBackward0>)\n",
      "Epoch 475, Training Loss:  tensor(149.9318, grad_fn=<SqrtBackward0>)\n",
      "Epoch 476, Training Loss:  tensor(263.9943, grad_fn=<SqrtBackward0>)\n",
      "Epoch 477, Training Loss:  tensor(141.0122, grad_fn=<SqrtBackward0>)\n",
      "Epoch 478, Training Loss:  tensor(219.0232, grad_fn=<SqrtBackward0>)\n",
      "Epoch 479, Training Loss:  tensor(111.2140, grad_fn=<SqrtBackward0>)\n",
      "Epoch 480, Training Loss:  tensor(238.9153, grad_fn=<SqrtBackward0>)\n",
      "Epoch 481, Training Loss:  tensor(188.5453, grad_fn=<SqrtBackward0>)\n",
      "Epoch 482, Training Loss:  tensor(211.9914, grad_fn=<SqrtBackward0>)\n",
      "Epoch 483, Training Loss:  tensor(196.6148, grad_fn=<SqrtBackward0>)\n",
      "Epoch 484, Training Loss:  tensor(171.0282, grad_fn=<SqrtBackward0>)\n",
      "Epoch 485, Training Loss:  tensor(229.3001, grad_fn=<SqrtBackward0>)\n",
      "Epoch 486, Training Loss:  tensor(208.2778, grad_fn=<SqrtBackward0>)\n",
      "Epoch 487, Training Loss:  tensor(120.0490, grad_fn=<SqrtBackward0>)\n",
      "Epoch 488, Training Loss:  tensor(202.0071, grad_fn=<SqrtBackward0>)\n",
      "Epoch 489, Training Loss:  tensor(191.5701, grad_fn=<SqrtBackward0>)\n",
      "Epoch 490, Training Loss:  tensor(153.2356, grad_fn=<SqrtBackward0>)\n",
      "Epoch 491, Training Loss:  tensor(205.7446, grad_fn=<SqrtBackward0>)\n",
      "Epoch 492, Training Loss:  tensor(167.7962, grad_fn=<SqrtBackward0>)\n",
      "Epoch 493, Training Loss:  tensor(193.7679, grad_fn=<SqrtBackward0>)\n",
      "Epoch 494, Training Loss:  tensor(157.3600, grad_fn=<SqrtBackward0>)\n",
      "Epoch 495, Training Loss:  tensor(173.4230, grad_fn=<SqrtBackward0>)\n",
      "Epoch 496, Training Loss:  tensor(185.1112, grad_fn=<SqrtBackward0>)\n",
      "Epoch 497, Training Loss:  tensor(178.0847, grad_fn=<SqrtBackward0>)\n",
      "Epoch 498, Training Loss:  tensor(117.5102, grad_fn=<SqrtBackward0>)\n",
      "Epoch 499, Training Loss:  tensor(246.7123, grad_fn=<SqrtBackward0>)\n",
      "Epoch 500, Training Loss:  tensor(149.1714, grad_fn=<SqrtBackward0>)\n",
      "Epoch 501, Training Loss:  tensor(159.0571, grad_fn=<SqrtBackward0>)\n",
      "Epoch 502, Training Loss:  tensor(178.8299, grad_fn=<SqrtBackward0>)\n",
      "Epoch 503, Training Loss:  tensor(221.0171, grad_fn=<SqrtBackward0>)\n",
      "Epoch 504, Training Loss:  tensor(166.8630, grad_fn=<SqrtBackward0>)\n",
      "Epoch 505, Training Loss:  tensor(146.8687, grad_fn=<SqrtBackward0>)\n",
      "Epoch 506, Training Loss:  tensor(163.0338, grad_fn=<SqrtBackward0>)\n",
      "Epoch 507, Training Loss:  tensor(164.1872, grad_fn=<SqrtBackward0>)\n",
      "Epoch 508, Training Loss:  tensor(230.8204, grad_fn=<SqrtBackward0>)\n",
      "Epoch 509, Training Loss:  tensor(134.7962, grad_fn=<SqrtBackward0>)\n",
      "Epoch 510, Training Loss:  tensor(111.1600, grad_fn=<SqrtBackward0>)\n",
      "Epoch 511, Training Loss:  tensor(137.1784, grad_fn=<SqrtBackward0>)\n",
      "Epoch 512, Training Loss:  tensor(168.5029, grad_fn=<SqrtBackward0>)\n",
      "Epoch 513, Training Loss:  tensor(122.9225, grad_fn=<SqrtBackward0>)\n",
      "Epoch 514, Training Loss:  tensor(155.4084, grad_fn=<SqrtBackward0>)\n",
      "Epoch 515, Training Loss:  tensor(104.2659, grad_fn=<SqrtBackward0>)\n",
      "Epoch 516, Training Loss:  tensor(123.2590, grad_fn=<SqrtBackward0>)\n",
      "Epoch 517, Training Loss:  tensor(246.1660, grad_fn=<SqrtBackward0>)\n",
      "Epoch 518, Training Loss:  tensor(121.1962, grad_fn=<SqrtBackward0>)\n",
      "Epoch 519, Training Loss:  tensor(214.5225, grad_fn=<SqrtBackward0>)\n",
      "Epoch 520, Training Loss:  tensor(154.9889, grad_fn=<SqrtBackward0>)\n",
      "Epoch 521, Training Loss:  tensor(163.8388, grad_fn=<SqrtBackward0>)\n",
      "Epoch 522, Training Loss:  tensor(172.8029, grad_fn=<SqrtBackward0>)\n",
      "Epoch 523, Training Loss:  tensor(159.8394, grad_fn=<SqrtBackward0>)\n",
      "Epoch 524, Training Loss:  tensor(188.0309, grad_fn=<SqrtBackward0>)\n",
      "Epoch 525, Training Loss:  tensor(162.2327, grad_fn=<SqrtBackward0>)\n",
      "Epoch 526, Training Loss:  tensor(152.7710, grad_fn=<SqrtBackward0>)\n",
      "Epoch 527, Training Loss:  tensor(192.9739, grad_fn=<SqrtBackward0>)\n",
      "Epoch 528, Training Loss:  tensor(144.1658, grad_fn=<SqrtBackward0>)\n",
      "Epoch 529, Training Loss:  tensor(203.6000, grad_fn=<SqrtBackward0>)\n",
      "Epoch 530, Training Loss:  tensor(134.6525, grad_fn=<SqrtBackward0>)\n",
      "Epoch 531, Training Loss:  tensor(158.9587, grad_fn=<SqrtBackward0>)\n",
      "Epoch 532, Training Loss:  tensor(175.9961, grad_fn=<SqrtBackward0>)\n",
      "Epoch 533, Training Loss:  tensor(163.1009, grad_fn=<SqrtBackward0>)\n",
      "Epoch 534, Training Loss:  tensor(162.0001, grad_fn=<SqrtBackward0>)\n",
      "Epoch 535, Training Loss:  tensor(137.0298, grad_fn=<SqrtBackward0>)\n",
      "Epoch 536, Training Loss:  tensor(176.1930, grad_fn=<SqrtBackward0>)\n",
      "Epoch 537, Training Loss:  tensor(210.1943, grad_fn=<SqrtBackward0>)\n",
      "Epoch 538, Training Loss:  tensor(158.6194, grad_fn=<SqrtBackward0>)\n",
      "Epoch 539, Training Loss:  tensor(193.5351, grad_fn=<SqrtBackward0>)\n",
      "Epoch 540, Training Loss:  tensor(200.5166, grad_fn=<SqrtBackward0>)\n",
      "Epoch 541, Training Loss:  tensor(210.1954, grad_fn=<SqrtBackward0>)\n",
      "Epoch 542, Training Loss:  tensor(71.2927, grad_fn=<SqrtBackward0>)\n",
      "Epoch 543, Training Loss:  tensor(107.3407, grad_fn=<SqrtBackward0>)\n",
      "Epoch 544, Training Loss:  tensor(152.9212, grad_fn=<SqrtBackward0>)\n",
      "Epoch 545, Training Loss:  tensor(242.5147, grad_fn=<SqrtBackward0>)\n",
      "Epoch 546, Training Loss:  tensor(199.5685, grad_fn=<SqrtBackward0>)\n",
      "Epoch 547, Training Loss:  tensor(122.7572, grad_fn=<SqrtBackward0>)\n",
      "Epoch 548, Training Loss:  tensor(95.7469, grad_fn=<SqrtBackward0>)\n",
      "Epoch 549, Training Loss:  tensor(137.2614, grad_fn=<SqrtBackward0>)\n",
      "Epoch 550, Training Loss:  tensor(134.5909, grad_fn=<SqrtBackward0>)\n",
      "Epoch 551, Training Loss:  tensor(197.5472, grad_fn=<SqrtBackward0>)\n",
      "Epoch 552, Training Loss:  tensor(192.1888, grad_fn=<SqrtBackward0>)\n",
      "Epoch 553, Training Loss:  tensor(303.2930, grad_fn=<SqrtBackward0>)\n",
      "Epoch 554, Training Loss:  tensor(88.7276, grad_fn=<SqrtBackward0>)\n",
      "Epoch 555, Training Loss:  tensor(86.8933, grad_fn=<SqrtBackward0>)\n",
      "Epoch 556, Training Loss:  tensor(73.6119, grad_fn=<SqrtBackward0>)\n",
      "Epoch 557, Training Loss:  tensor(102.3486, grad_fn=<SqrtBackward0>)\n",
      "Epoch 558, Training Loss:  tensor(97.9025, grad_fn=<SqrtBackward0>)\n",
      "Epoch 559, Training Loss:  tensor(99.7850, grad_fn=<SqrtBackward0>)\n",
      "Epoch 560, Training Loss:  tensor(141.2352, grad_fn=<SqrtBackward0>)\n",
      "Epoch 561, Training Loss:  tensor(75.9063, grad_fn=<SqrtBackward0>)\n",
      "Epoch 562, Training Loss:  tensor(243.6400, grad_fn=<SqrtBackward0>)\n",
      "Epoch 563, Training Loss:  tensor(94.4988, grad_fn=<SqrtBackward0>)\n",
      "Epoch 564, Training Loss:  tensor(88.3220, grad_fn=<SqrtBackward0>)\n",
      "Epoch 565, Training Loss:  tensor(275.0492, grad_fn=<SqrtBackward0>)\n",
      "Epoch 566, Training Loss:  tensor(203.2006, grad_fn=<SqrtBackward0>)\n",
      "Epoch 567, Training Loss:  tensor(93.8273, grad_fn=<SqrtBackward0>)\n",
      "Epoch 568, Training Loss:  tensor(100.5011, grad_fn=<SqrtBackward0>)\n",
      "Epoch 569, Training Loss:  tensor(191.6340, grad_fn=<SqrtBackward0>)\n",
      "Epoch 570, Training Loss:  tensor(63.9360, grad_fn=<SqrtBackward0>)\n",
      "Epoch 571, Training Loss:  tensor(76.1688, grad_fn=<SqrtBackward0>)\n",
      "Epoch 572, Training Loss:  tensor(127.7099, grad_fn=<SqrtBackward0>)\n",
      "Epoch 573, Training Loss:  tensor(55.0553, grad_fn=<SqrtBackward0>)\n",
      "Epoch 574, Training Loss:  tensor(109.6776, grad_fn=<SqrtBackward0>)\n",
      "Epoch 575, Training Loss:  tensor(139.0149, grad_fn=<SqrtBackward0>)\n",
      "Epoch 576, Training Loss:  tensor(108.1656, grad_fn=<SqrtBackward0>)\n",
      "Epoch 577, Training Loss:  tensor(157.4330, grad_fn=<SqrtBackward0>)\n",
      "Epoch 578, Training Loss:  tensor(253.6201, grad_fn=<SqrtBackward0>)\n",
      "Epoch 579, Training Loss:  tensor(133.0412, grad_fn=<SqrtBackward0>)\n",
      "Epoch 580, Training Loss:  tensor(107.5537, grad_fn=<SqrtBackward0>)\n",
      "Epoch 581, Training Loss:  tensor(92.2684, grad_fn=<SqrtBackward0>)\n",
      "Epoch 582, Training Loss:  tensor(47.0396, grad_fn=<SqrtBackward0>)\n",
      "Epoch 583, Training Loss:  tensor(181.5234, grad_fn=<SqrtBackward0>)\n",
      "Epoch 584, Training Loss:  tensor(149.7866, grad_fn=<SqrtBackward0>)\n",
      "Epoch 585, Training Loss:  tensor(96.8553, grad_fn=<SqrtBackward0>)\n",
      "Epoch 586, Training Loss:  tensor(210.9194, grad_fn=<SqrtBackward0>)\n",
      "Epoch 587, Training Loss:  tensor(170.0627, grad_fn=<SqrtBackward0>)\n",
      "Epoch 588, Training Loss:  tensor(211.3902, grad_fn=<SqrtBackward0>)\n",
      "Epoch 589, Training Loss:  tensor(189.3082, grad_fn=<SqrtBackward0>)\n",
      "Epoch 590, Training Loss:  tensor(114.5193, grad_fn=<SqrtBackward0>)\n",
      "Epoch 591, Training Loss:  tensor(162.1054, grad_fn=<SqrtBackward0>)\n",
      "Epoch 592, Training Loss:  tensor(132.1495, grad_fn=<SqrtBackward0>)\n",
      "Epoch 593, Training Loss:  tensor(80.7595, grad_fn=<SqrtBackward0>)\n",
      "Epoch 594, Training Loss:  tensor(98.3968, grad_fn=<SqrtBackward0>)\n",
      "Epoch 595, Training Loss:  tensor(119.4818, grad_fn=<SqrtBackward0>)\n",
      "Epoch 596, Training Loss:  tensor(136.7147, grad_fn=<SqrtBackward0>)\n",
      "Epoch 597, Training Loss:  tensor(117.1342, grad_fn=<SqrtBackward0>)\n",
      "Epoch 598, Training Loss:  tensor(154.6057, grad_fn=<SqrtBackward0>)\n",
      "Epoch 599, Training Loss:  tensor(71.7043, grad_fn=<SqrtBackward0>)\n",
      "Epoch 600, Training Loss:  tensor(173.3152, grad_fn=<SqrtBackward0>)\n",
      "Epoch 601, Training Loss:  tensor(217.3983, grad_fn=<SqrtBackward0>)\n",
      "Epoch 602, Training Loss:  tensor(198.9606, grad_fn=<SqrtBackward0>)\n",
      "Epoch 603, Training Loss:  tensor(119.3603, grad_fn=<SqrtBackward0>)\n",
      "Epoch 604, Training Loss:  tensor(181.5089, grad_fn=<SqrtBackward0>)\n",
      "Epoch 605, Training Loss:  tensor(154.5536, grad_fn=<SqrtBackward0>)\n",
      "Epoch 606, Training Loss:  tensor(184.7768, grad_fn=<SqrtBackward0>)\n",
      "Epoch 607, Training Loss:  tensor(106.2953, grad_fn=<SqrtBackward0>)\n",
      "Epoch 608, Training Loss:  tensor(161.3846, grad_fn=<SqrtBackward0>)\n",
      "Epoch 609, Training Loss:  tensor(183.9211, grad_fn=<SqrtBackward0>)\n",
      "Epoch 610, Training Loss:  tensor(111.6056, grad_fn=<SqrtBackward0>)\n",
      "Epoch 611, Training Loss:  tensor(234.2358, grad_fn=<SqrtBackward0>)\n",
      "Epoch 612, Training Loss:  tensor(306.1488, grad_fn=<SqrtBackward0>)\n",
      "Epoch 613, Training Loss:  tensor(53.4516, grad_fn=<SqrtBackward0>)\n",
      "Epoch 614, Training Loss:  tensor(261.5876, grad_fn=<SqrtBackward0>)\n",
      "Epoch 615, Training Loss:  tensor(326.8287, grad_fn=<SqrtBackward0>)\n",
      "Epoch 616, Training Loss:  tensor(152.0470, grad_fn=<SqrtBackward0>)\n",
      "Epoch 617, Training Loss:  tensor(109.9009, grad_fn=<SqrtBackward0>)\n",
      "Epoch 618, Training Loss:  tensor(115.8426, grad_fn=<SqrtBackward0>)\n",
      "Epoch 619, Training Loss:  tensor(152.8060, grad_fn=<SqrtBackward0>)\n",
      "Epoch 620, Training Loss:  tensor(107.7931, grad_fn=<SqrtBackward0>)\n",
      "Epoch 621, Training Loss:  tensor(135.1664, grad_fn=<SqrtBackward0>)\n",
      "Epoch 622, Training Loss:  tensor(193.1830, grad_fn=<SqrtBackward0>)\n",
      "Epoch 623, Training Loss:  tensor(95.2206, grad_fn=<SqrtBackward0>)\n",
      "Epoch 624, Training Loss:  tensor(74.9495, grad_fn=<SqrtBackward0>)\n",
      "Epoch 625, Training Loss:  tensor(191.1673, grad_fn=<SqrtBackward0>)\n",
      "Epoch 626, Training Loss:  tensor(274.7323, grad_fn=<SqrtBackward0>)\n",
      "Epoch 627, Training Loss:  tensor(289.2049, grad_fn=<SqrtBackward0>)\n",
      "Epoch 628, Training Loss:  tensor(311.1867, grad_fn=<SqrtBackward0>)\n",
      "Epoch 629, Training Loss:  tensor(151.6388, grad_fn=<SqrtBackward0>)\n",
      "Epoch 630, Training Loss:  tensor(125.1151, grad_fn=<SqrtBackward0>)\n",
      "Epoch 631, Training Loss:  tensor(250.1925, grad_fn=<SqrtBackward0>)\n",
      "Epoch 632, Training Loss:  tensor(190.3666, grad_fn=<SqrtBackward0>)\n",
      "Epoch 633, Training Loss:  tensor(142.8231, grad_fn=<SqrtBackward0>)\n",
      "Epoch 634, Training Loss:  tensor(188.1331, grad_fn=<SqrtBackward0>)\n",
      "Epoch 635, Training Loss:  tensor(199.4044, grad_fn=<SqrtBackward0>)\n",
      "Epoch 636, Training Loss:  tensor(42.7810, grad_fn=<SqrtBackward0>)\n",
      "Epoch 637, Training Loss:  tensor(101.4254, grad_fn=<SqrtBackward0>)\n",
      "Epoch 638, Training Loss:  tensor(252.3864, grad_fn=<SqrtBackward0>)\n",
      "Epoch 639, Training Loss:  tensor(108.6035, grad_fn=<SqrtBackward0>)\n",
      "Epoch 640, Training Loss:  tensor(164.7515, grad_fn=<SqrtBackward0>)\n",
      "Epoch 641, Training Loss:  tensor(92.9500, grad_fn=<SqrtBackward0>)\n",
      "Epoch 642, Training Loss:  tensor(155.9896, grad_fn=<SqrtBackward0>)\n",
      "Epoch 643, Training Loss:  tensor(183.1617, grad_fn=<SqrtBackward0>)\n",
      "Epoch 644, Training Loss:  tensor(140.4084, grad_fn=<SqrtBackward0>)\n",
      "Epoch 645, Training Loss:  tensor(204.8436, grad_fn=<SqrtBackward0>)\n",
      "Epoch 646, Training Loss:  tensor(211.8796, grad_fn=<SqrtBackward0>)\n",
      "Epoch 647, Training Loss:  tensor(195.7187, grad_fn=<SqrtBackward0>)\n",
      "Epoch 648, Training Loss:  tensor(144.0897, grad_fn=<SqrtBackward0>)\n",
      "Epoch 649, Training Loss:  tensor(125.9818, grad_fn=<SqrtBackward0>)\n",
      "Epoch 650, Training Loss:  tensor(164.5349, grad_fn=<SqrtBackward0>)\n",
      "Epoch 651, Training Loss:  tensor(61.1293, grad_fn=<SqrtBackward0>)\n",
      "Epoch 652, Training Loss:  tensor(130.6182, grad_fn=<SqrtBackward0>)\n",
      "Epoch 653, Training Loss:  tensor(121.0189, grad_fn=<SqrtBackward0>)\n",
      "Epoch 654, Training Loss:  tensor(132.6779, grad_fn=<SqrtBackward0>)\n",
      "Epoch 655, Training Loss:  tensor(198.1123, grad_fn=<SqrtBackward0>)\n",
      "Epoch 656, Training Loss:  tensor(205.1190, grad_fn=<SqrtBackward0>)\n",
      "Epoch 657, Training Loss:  tensor(192.0775, grad_fn=<SqrtBackward0>)\n",
      "Epoch 658, Training Loss:  tensor(406.1863, grad_fn=<SqrtBackward0>)\n",
      "Epoch 659, Training Loss:  tensor(91.3643, grad_fn=<SqrtBackward0>)\n",
      "Epoch 660, Training Loss:  tensor(64.7789, grad_fn=<SqrtBackward0>)\n",
      "Epoch 661, Training Loss:  tensor(148.5543, grad_fn=<SqrtBackward0>)\n",
      "Epoch 662, Training Loss:  tensor(132.4128, grad_fn=<SqrtBackward0>)\n",
      "Epoch 663, Training Loss:  tensor(162.4274, grad_fn=<SqrtBackward0>)\n",
      "Epoch 664, Training Loss:  tensor(227.0118, grad_fn=<SqrtBackward0>)\n",
      "Epoch 665, Training Loss:  tensor(173.3931, grad_fn=<SqrtBackward0>)\n",
      "Epoch 666, Training Loss:  tensor(87.7660, grad_fn=<SqrtBackward0>)\n",
      "Epoch 667, Training Loss:  tensor(137.1360, grad_fn=<SqrtBackward0>)\n",
      "Epoch 668, Training Loss:  tensor(235.2387, grad_fn=<SqrtBackward0>)\n",
      "Epoch 669, Training Loss:  tensor(199.4812, grad_fn=<SqrtBackward0>)\n",
      "Epoch 670, Training Loss:  tensor(229.9598, grad_fn=<SqrtBackward0>)\n",
      "Epoch 671, Training Loss:  tensor(230.0300, grad_fn=<SqrtBackward0>)\n",
      "Epoch 672, Training Loss:  tensor(189.6922, grad_fn=<SqrtBackward0>)\n",
      "Epoch 673, Training Loss:  tensor(67.0768, grad_fn=<SqrtBackward0>)\n",
      "Epoch 674, Training Loss:  tensor(184.5948, grad_fn=<SqrtBackward0>)\n",
      "Epoch 675, Training Loss:  tensor(214.1534, grad_fn=<SqrtBackward0>)\n",
      "Epoch 676, Training Loss:  tensor(140.6620, grad_fn=<SqrtBackward0>)\n",
      "Epoch 677, Training Loss:  tensor(86.9920, grad_fn=<SqrtBackward0>)\n",
      "Epoch 678, Training Loss:  tensor(161.0204, grad_fn=<SqrtBackward0>)\n",
      "Epoch 679, Training Loss:  tensor(294.2750, grad_fn=<SqrtBackward0>)\n",
      "Epoch 680, Training Loss:  tensor(98.6893, grad_fn=<SqrtBackward0>)\n",
      "Epoch 681, Training Loss:  tensor(88.3378, grad_fn=<SqrtBackward0>)\n",
      "Epoch 682, Training Loss:  tensor(156.1194, grad_fn=<SqrtBackward0>)\n",
      "Epoch 683, Training Loss:  tensor(266.9326, grad_fn=<SqrtBackward0>)\n",
      "Epoch 684, Training Loss:  tensor(172.3627, grad_fn=<SqrtBackward0>)\n",
      "Epoch 685, Training Loss:  tensor(158.8485, grad_fn=<SqrtBackward0>)\n",
      "Epoch 686, Training Loss:  tensor(170.7957, grad_fn=<SqrtBackward0>)\n",
      "Epoch 687, Training Loss:  tensor(252.3314, grad_fn=<SqrtBackward0>)\n",
      "Epoch 688, Training Loss:  tensor(123.2025, grad_fn=<SqrtBackward0>)\n",
      "Epoch 689, Training Loss:  tensor(175.3002, grad_fn=<SqrtBackward0>)\n",
      "Epoch 690, Training Loss:  tensor(220.3448, grad_fn=<SqrtBackward0>)\n",
      "Epoch 691, Training Loss:  tensor(256.7316, grad_fn=<SqrtBackward0>)\n",
      "Epoch 692, Training Loss:  tensor(153.5125, grad_fn=<SqrtBackward0>)\n",
      "Epoch 693, Training Loss:  tensor(123.7045, grad_fn=<SqrtBackward0>)\n",
      "Epoch 694, Training Loss:  tensor(190.1247, grad_fn=<SqrtBackward0>)\n",
      "Epoch 695, Training Loss:  tensor(131.8798, grad_fn=<SqrtBackward0>)\n",
      "Epoch 696, Training Loss:  tensor(181.0191, grad_fn=<SqrtBackward0>)\n",
      "Epoch 697, Training Loss:  tensor(212.3400, grad_fn=<SqrtBackward0>)\n",
      "Epoch 698, Training Loss:  tensor(160.7360, grad_fn=<SqrtBackward0>)\n",
      "Epoch 699, Training Loss:  tensor(44.7004, grad_fn=<SqrtBackward0>)\n",
      "Epoch 700, Training Loss:  tensor(198.5437, grad_fn=<SqrtBackward0>)\n",
      "Epoch 701, Training Loss:  tensor(142.2608, grad_fn=<SqrtBackward0>)\n",
      "Epoch 702, Training Loss:  tensor(115.6187, grad_fn=<SqrtBackward0>)\n",
      "Epoch 703, Training Loss:  tensor(140.2894, grad_fn=<SqrtBackward0>)\n",
      "Epoch 704, Training Loss:  tensor(263.3680, grad_fn=<SqrtBackward0>)\n",
      "Epoch 705, Training Loss:  tensor(156.4241, grad_fn=<SqrtBackward0>)\n",
      "Epoch 706, Training Loss:  tensor(158.5439, grad_fn=<SqrtBackward0>)\n",
      "Epoch 707, Training Loss:  tensor(72.3488, grad_fn=<SqrtBackward0>)\n",
      "Epoch 708, Training Loss:  tensor(153.8287, grad_fn=<SqrtBackward0>)\n",
      "Epoch 709, Training Loss:  tensor(276.4997, grad_fn=<SqrtBackward0>)\n",
      "Epoch 710, Training Loss:  tensor(321.1850, grad_fn=<SqrtBackward0>)\n",
      "Epoch 711, Training Loss:  tensor(153.1968, grad_fn=<SqrtBackward0>)\n",
      "Epoch 712, Training Loss:  tensor(286.1703, grad_fn=<SqrtBackward0>)\n",
      "Epoch 713, Training Loss:  tensor(186.1179, grad_fn=<SqrtBackward0>)\n",
      "Epoch 714, Training Loss:  tensor(312.4438, grad_fn=<SqrtBackward0>)\n",
      "Epoch 715, Training Loss:  tensor(333.7837, grad_fn=<SqrtBackward0>)\n",
      "Epoch 716, Training Loss:  tensor(148.9623, grad_fn=<SqrtBackward0>)\n",
      "Epoch 717, Training Loss:  tensor(145.0637, grad_fn=<SqrtBackward0>)\n",
      "Epoch 718, Training Loss:  tensor(150.2608, grad_fn=<SqrtBackward0>)\n",
      "Epoch 719, Training Loss:  tensor(66.5151, grad_fn=<SqrtBackward0>)\n",
      "Epoch 720, Training Loss:  tensor(145.1872, grad_fn=<SqrtBackward0>)\n",
      "Epoch 721, Training Loss:  tensor(150.8950, grad_fn=<SqrtBackward0>)\n",
      "Epoch 722, Training Loss:  tensor(150.2452, grad_fn=<SqrtBackward0>)\n",
      "Epoch 723, Training Loss:  tensor(198.3220, grad_fn=<SqrtBackward0>)\n",
      "Epoch 724, Training Loss:  tensor(119.3417, grad_fn=<SqrtBackward0>)\n",
      "Epoch 725, Training Loss:  tensor(114.1557, grad_fn=<SqrtBackward0>)\n",
      "Epoch 726, Training Loss:  tensor(143.0436, grad_fn=<SqrtBackward0>)\n",
      "Epoch 727, Training Loss:  tensor(215.8186, grad_fn=<SqrtBackward0>)\n",
      "Epoch 728, Training Loss:  tensor(105.9432, grad_fn=<SqrtBackward0>)\n",
      "Epoch 729, Training Loss:  tensor(139.1566, grad_fn=<SqrtBackward0>)\n",
      "Epoch 730, Training Loss:  tensor(233.7780, grad_fn=<SqrtBackward0>)\n",
      "Epoch 731, Training Loss:  tensor(144.4564, grad_fn=<SqrtBackward0>)\n",
      "Epoch 732, Training Loss:  tensor(128.2549, grad_fn=<SqrtBackward0>)\n",
      "Epoch 733, Training Loss:  tensor(229.8404, grad_fn=<SqrtBackward0>)\n",
      "Epoch 734, Training Loss:  tensor(112.0837, grad_fn=<SqrtBackward0>)\n",
      "Epoch 735, Training Loss:  tensor(166.0412, grad_fn=<SqrtBackward0>)\n",
      "Epoch 736, Training Loss:  tensor(183.7238, grad_fn=<SqrtBackward0>)\n",
      "Epoch 737, Training Loss:  tensor(136.9311, grad_fn=<SqrtBackward0>)\n",
      "Epoch 738, Training Loss:  tensor(167.4367, grad_fn=<SqrtBackward0>)\n",
      "Epoch 739, Training Loss:  tensor(101.5268, grad_fn=<SqrtBackward0>)\n",
      "Epoch 740, Training Loss:  tensor(109.6744, grad_fn=<SqrtBackward0>)\n",
      "Epoch 741, Training Loss:  tensor(167.7897, grad_fn=<SqrtBackward0>)\n",
      "Epoch 742, Training Loss:  tensor(177.9664, grad_fn=<SqrtBackward0>)\n",
      "Epoch 743, Training Loss:  tensor(137.2071, grad_fn=<SqrtBackward0>)\n",
      "Epoch 744, Training Loss:  tensor(229.4956, grad_fn=<SqrtBackward0>)\n",
      "Epoch 745, Training Loss:  tensor(76.0519, grad_fn=<SqrtBackward0>)\n",
      "Epoch 746, Training Loss:  tensor(127.3757, grad_fn=<SqrtBackward0>)\n",
      "Epoch 747, Training Loss:  tensor(206.3619, grad_fn=<SqrtBackward0>)\n",
      "Epoch 748, Training Loss:  tensor(48.8224, grad_fn=<SqrtBackward0>)\n",
      "Epoch 749, Training Loss:  tensor(203.2237, grad_fn=<SqrtBackward0>)\n",
      "Epoch 750, Training Loss:  tensor(135.0157, grad_fn=<SqrtBackward0>)\n",
      "Epoch 751, Training Loss:  tensor(272.5960, grad_fn=<SqrtBackward0>)\n",
      "Epoch 752, Training Loss:  tensor(141.3941, grad_fn=<SqrtBackward0>)\n",
      "Epoch 753, Training Loss:  tensor(88.2114, grad_fn=<SqrtBackward0>)\n",
      "Epoch 754, Training Loss:  tensor(149.5327, grad_fn=<SqrtBackward0>)\n",
      "Epoch 755, Training Loss:  tensor(119.8841, grad_fn=<SqrtBackward0>)\n",
      "Epoch 756, Training Loss:  tensor(144.9061, grad_fn=<SqrtBackward0>)\n",
      "Epoch 757, Training Loss:  tensor(114.7542, grad_fn=<SqrtBackward0>)\n",
      "Epoch 758, Training Loss:  tensor(170.6001, grad_fn=<SqrtBackward0>)\n",
      "Epoch 759, Training Loss:  tensor(143.0429, grad_fn=<SqrtBackward0>)\n",
      "Epoch 760, Training Loss:  tensor(121.9510, grad_fn=<SqrtBackward0>)\n",
      "Epoch 761, Training Loss:  tensor(141.8618, grad_fn=<SqrtBackward0>)\n",
      "Epoch 762, Training Loss:  tensor(68.7122, grad_fn=<SqrtBackward0>)\n",
      "Epoch 763, Training Loss:  tensor(73.2990, grad_fn=<SqrtBackward0>)\n",
      "Epoch 764, Training Loss:  tensor(123.3839, grad_fn=<SqrtBackward0>)\n",
      "Epoch 765, Training Loss:  tensor(114.2152, grad_fn=<SqrtBackward0>)\n",
      "Epoch 766, Training Loss:  tensor(134.9530, grad_fn=<SqrtBackward0>)\n",
      "Epoch 767, Training Loss:  tensor(210.2749, grad_fn=<SqrtBackward0>)\n",
      "Epoch 768, Training Loss:  tensor(158.6761, grad_fn=<SqrtBackward0>)\n",
      "Epoch 769, Training Loss:  tensor(169.3370, grad_fn=<SqrtBackward0>)\n",
      "Epoch 770, Training Loss:  tensor(195.3006, grad_fn=<SqrtBackward0>)\n",
      "Epoch 771, Training Loss:  tensor(124.7911, grad_fn=<SqrtBackward0>)\n",
      "Epoch 772, Training Loss:  tensor(89.2500, grad_fn=<SqrtBackward0>)\n",
      "Epoch 773, Training Loss:  tensor(208.7154, grad_fn=<SqrtBackward0>)\n",
      "Epoch 774, Training Loss:  tensor(137.0509, grad_fn=<SqrtBackward0>)\n",
      "Epoch 775, Training Loss:  tensor(59.9712, grad_fn=<SqrtBackward0>)\n",
      "Epoch 776, Training Loss:  tensor(68.5582, grad_fn=<SqrtBackward0>)\n",
      "Epoch 777, Training Loss:  tensor(167.4072, grad_fn=<SqrtBackward0>)\n",
      "Epoch 778, Training Loss:  tensor(167.2027, grad_fn=<SqrtBackward0>)\n",
      "Epoch 779, Training Loss:  tensor(191.5346, grad_fn=<SqrtBackward0>)\n",
      "Epoch 780, Training Loss:  tensor(192.6510, grad_fn=<SqrtBackward0>)\n",
      "Epoch 781, Training Loss:  tensor(181.1979, grad_fn=<SqrtBackward0>)\n",
      "Epoch 782, Training Loss:  tensor(235.2725, grad_fn=<SqrtBackward0>)\n",
      "Epoch 783, Training Loss:  tensor(224.8934, grad_fn=<SqrtBackward0>)\n",
      "Epoch 784, Training Loss:  tensor(151.7491, grad_fn=<SqrtBackward0>)\n",
      "Epoch 785, Training Loss:  tensor(223.1125, grad_fn=<SqrtBackward0>)\n",
      "Epoch 786, Training Loss:  tensor(114.6405, grad_fn=<SqrtBackward0>)\n",
      "Epoch 787, Training Loss:  tensor(167.8269, grad_fn=<SqrtBackward0>)\n",
      "Epoch 788, Training Loss:  tensor(172.4837, grad_fn=<SqrtBackward0>)\n",
      "Epoch 789, Training Loss:  tensor(114.8425, grad_fn=<SqrtBackward0>)\n",
      "Epoch 790, Training Loss:  tensor(87.7968, grad_fn=<SqrtBackward0>)\n",
      "Epoch 791, Training Loss:  tensor(267.5761, grad_fn=<SqrtBackward0>)\n",
      "Epoch 792, Training Loss:  tensor(148.4701, grad_fn=<SqrtBackward0>)\n",
      "Epoch 793, Training Loss:  tensor(101.1234, grad_fn=<SqrtBackward0>)\n",
      "Epoch 794, Training Loss:  tensor(155.2267, grad_fn=<SqrtBackward0>)\n",
      "Epoch 795, Training Loss:  tensor(175.2542, grad_fn=<SqrtBackward0>)\n",
      "Epoch 796, Training Loss:  tensor(191.3883, grad_fn=<SqrtBackward0>)\n",
      "Epoch 797, Training Loss:  tensor(114.0096, grad_fn=<SqrtBackward0>)\n",
      "Epoch 798, Training Loss:  tensor(175.7145, grad_fn=<SqrtBackward0>)\n",
      "Epoch 799, Training Loss:  tensor(466.6318, grad_fn=<SqrtBackward0>)\n",
      "Epoch 800, Training Loss:  tensor(245.7555, grad_fn=<SqrtBackward0>)\n",
      "Epoch 801, Training Loss:  tensor(178.1945, grad_fn=<SqrtBackward0>)\n",
      "Epoch 802, Training Loss:  tensor(166.3318, grad_fn=<SqrtBackward0>)\n",
      "Epoch 803, Training Loss:  tensor(419.4075, grad_fn=<SqrtBackward0>)\n",
      "Epoch 804, Training Loss:  tensor(560.5151, grad_fn=<SqrtBackward0>)\n",
      "Epoch 805, Training Loss:  tensor(317.6650, grad_fn=<SqrtBackward0>)\n",
      "Epoch 806, Training Loss:  tensor(371.0019, grad_fn=<SqrtBackward0>)\n",
      "Epoch 807, Training Loss:  tensor(69.8994, grad_fn=<SqrtBackward0>)\n",
      "Epoch 808, Training Loss:  tensor(155.3713, grad_fn=<SqrtBackward0>)\n",
      "Epoch 809, Training Loss:  tensor(217.3167, grad_fn=<SqrtBackward0>)\n",
      "Epoch 810, Training Loss:  tensor(145.3917, grad_fn=<SqrtBackward0>)\n",
      "Epoch 811, Training Loss:  tensor(224.9360, grad_fn=<SqrtBackward0>)\n",
      "Epoch 812, Training Loss:  tensor(295.5145, grad_fn=<SqrtBackward0>)\n",
      "Epoch 813, Training Loss:  tensor(208.1235, grad_fn=<SqrtBackward0>)\n",
      "Epoch 814, Training Loss:  tensor(129.5916, grad_fn=<SqrtBackward0>)\n",
      "Epoch 815, Training Loss:  tensor(101.7665, grad_fn=<SqrtBackward0>)\n",
      "Epoch 816, Training Loss:  tensor(207.5693, grad_fn=<SqrtBackward0>)\n",
      "Epoch 817, Training Loss:  tensor(143.2026, grad_fn=<SqrtBackward0>)\n",
      "Epoch 818, Training Loss:  tensor(231.0794, grad_fn=<SqrtBackward0>)\n",
      "Epoch 819, Training Loss:  tensor(118.3309, grad_fn=<SqrtBackward0>)\n",
      "Epoch 820, Training Loss:  tensor(152.5632, grad_fn=<SqrtBackward0>)\n",
      "Epoch 821, Training Loss:  tensor(170.3827, grad_fn=<SqrtBackward0>)\n",
      "Epoch 822, Training Loss:  tensor(144.9742, grad_fn=<SqrtBackward0>)\n",
      "Epoch 823, Training Loss:  tensor(139.0266, grad_fn=<SqrtBackward0>)\n",
      "Epoch 824, Training Loss:  tensor(158.4388, grad_fn=<SqrtBackward0>)\n",
      "Epoch 825, Training Loss:  tensor(114.1558, grad_fn=<SqrtBackward0>)\n",
      "Epoch 826, Training Loss:  tensor(119.1264, grad_fn=<SqrtBackward0>)\n",
      "Epoch 827, Training Loss:  tensor(101.5100, grad_fn=<SqrtBackward0>)\n",
      "Epoch 828, Training Loss:  tensor(219.3247, grad_fn=<SqrtBackward0>)\n",
      "Epoch 829, Training Loss:  tensor(105.5757, grad_fn=<SqrtBackward0>)\n",
      "Epoch 830, Training Loss:  tensor(148.9904, grad_fn=<SqrtBackward0>)\n",
      "Epoch 831, Training Loss:  tensor(144.4790, grad_fn=<SqrtBackward0>)\n",
      "Epoch 832, Training Loss:  tensor(90.6170, grad_fn=<SqrtBackward0>)\n",
      "Epoch 833, Training Loss:  tensor(218.4471, grad_fn=<SqrtBackward0>)\n",
      "Epoch 834, Training Loss:  tensor(195.1188, grad_fn=<SqrtBackward0>)\n",
      "Epoch 835, Training Loss:  tensor(176.5919, grad_fn=<SqrtBackward0>)\n",
      "Epoch 836, Training Loss:  tensor(123.7616, grad_fn=<SqrtBackward0>)\n",
      "Epoch 837, Training Loss:  tensor(240.0271, grad_fn=<SqrtBackward0>)\n",
      "Epoch 838, Training Loss:  tensor(126.3566, grad_fn=<SqrtBackward0>)\n",
      "Epoch 839, Training Loss:  tensor(99.4841, grad_fn=<SqrtBackward0>)\n",
      "Epoch 840, Training Loss:  tensor(39.0749, grad_fn=<SqrtBackward0>)\n",
      "Epoch 841, Training Loss:  tensor(233.3899, grad_fn=<SqrtBackward0>)\n",
      "Epoch 842, Training Loss:  tensor(166.8374, grad_fn=<SqrtBackward0>)\n",
      "Epoch 843, Training Loss:  tensor(219.8593, grad_fn=<SqrtBackward0>)\n",
      "Epoch 844, Training Loss:  tensor(194.4123, grad_fn=<SqrtBackward0>)\n",
      "Epoch 845, Training Loss:  tensor(277.4725, grad_fn=<SqrtBackward0>)\n",
      "Epoch 846, Training Loss:  tensor(238.6843, grad_fn=<SqrtBackward0>)\n",
      "Epoch 847, Training Loss:  tensor(200.8105, grad_fn=<SqrtBackward0>)\n",
      "Epoch 848, Training Loss:  tensor(256.2118, grad_fn=<SqrtBackward0>)\n",
      "Epoch 849, Training Loss:  tensor(255.1676, grad_fn=<SqrtBackward0>)\n",
      "Epoch 850, Training Loss:  tensor(143.7270, grad_fn=<SqrtBackward0>)\n",
      "Epoch 851, Training Loss:  tensor(212.2915, grad_fn=<SqrtBackward0>)\n",
      "Epoch 852, Training Loss:  tensor(215.6169, grad_fn=<SqrtBackward0>)\n",
      "Epoch 853, Training Loss:  tensor(118.0874, grad_fn=<SqrtBackward0>)\n",
      "Epoch 854, Training Loss:  tensor(294.1938, grad_fn=<SqrtBackward0>)\n",
      "Epoch 855, Training Loss:  tensor(322.5309, grad_fn=<SqrtBackward0>)\n",
      "Epoch 856, Training Loss:  tensor(60.8478, grad_fn=<SqrtBackward0>)\n",
      "Epoch 857, Training Loss:  tensor(329.5481, grad_fn=<SqrtBackward0>)\n",
      "Epoch 858, Training Loss:  tensor(135.9520, grad_fn=<SqrtBackward0>)\n",
      "Epoch 859, Training Loss:  tensor(247.5781, grad_fn=<SqrtBackward0>)\n",
      "Epoch 860, Training Loss:  tensor(307.8548, grad_fn=<SqrtBackward0>)\n",
      "Epoch 861, Training Loss:  tensor(120.4378, grad_fn=<SqrtBackward0>)\n",
      "Epoch 862, Training Loss:  tensor(255.0086, grad_fn=<SqrtBackward0>)\n",
      "Epoch 863, Training Loss:  tensor(331.7370, grad_fn=<SqrtBackward0>)\n",
      "Epoch 864, Training Loss:  tensor(159.9563, grad_fn=<SqrtBackward0>)\n",
      "Epoch 865, Training Loss:  tensor(190.5938, grad_fn=<SqrtBackward0>)\n",
      "Epoch 866, Training Loss:  tensor(244.4674, grad_fn=<SqrtBackward0>)\n",
      "Epoch 867, Training Loss:  tensor(126.9000, grad_fn=<SqrtBackward0>)\n",
      "Epoch 868, Training Loss:  tensor(248.7319, grad_fn=<SqrtBackward0>)\n",
      "Epoch 869, Training Loss:  tensor(109.1779, grad_fn=<SqrtBackward0>)\n",
      "Epoch 870, Training Loss:  tensor(136.2115, grad_fn=<SqrtBackward0>)\n",
      "Epoch 871, Training Loss:  tensor(182.2030, grad_fn=<SqrtBackward0>)\n",
      "Epoch 872, Training Loss:  tensor(236.7384, grad_fn=<SqrtBackward0>)\n",
      "Epoch 873, Training Loss:  tensor(277.6142, grad_fn=<SqrtBackward0>)\n",
      "Epoch 874, Training Loss:  tensor(326.3110, grad_fn=<SqrtBackward0>)\n",
      "Epoch 875, Training Loss:  tensor(148.5530, grad_fn=<SqrtBackward0>)\n",
      "Epoch 876, Training Loss:  tensor(144.8488, grad_fn=<SqrtBackward0>)\n",
      "Epoch 877, Training Loss:  tensor(443.2042, grad_fn=<SqrtBackward0>)\n",
      "Epoch 878, Training Loss:  tensor(280.7799, grad_fn=<SqrtBackward0>)\n",
      "Epoch 879, Training Loss:  tensor(159.6858, grad_fn=<SqrtBackward0>)\n",
      "Epoch 880, Training Loss:  tensor(182.6171, grad_fn=<SqrtBackward0>)\n",
      "Epoch 881, Training Loss:  tensor(134.6579, grad_fn=<SqrtBackward0>)\n",
      "Epoch 882, Training Loss:  tensor(156.9491, grad_fn=<SqrtBackward0>)\n",
      "Epoch 883, Training Loss:  tensor(129.3865, grad_fn=<SqrtBackward0>)\n",
      "Epoch 884, Training Loss:  tensor(84.7417, grad_fn=<SqrtBackward0>)\n",
      "Epoch 885, Training Loss:  tensor(38.6243, grad_fn=<SqrtBackward0>)\n",
      "Epoch 886, Training Loss:  tensor(220.2614, grad_fn=<SqrtBackward0>)\n",
      "Epoch 887, Training Loss:  tensor(104.4945, grad_fn=<SqrtBackward0>)\n",
      "Epoch 888, Training Loss:  tensor(269.1539, grad_fn=<SqrtBackward0>)\n",
      "Epoch 889, Training Loss:  tensor(233.8486, grad_fn=<SqrtBackward0>)\n",
      "Epoch 890, Training Loss:  tensor(311.6756, grad_fn=<SqrtBackward0>)\n",
      "Epoch 891, Training Loss:  tensor(84.8220, grad_fn=<SqrtBackward0>)\n",
      "Epoch 892, Training Loss:  tensor(128.4706, grad_fn=<SqrtBackward0>)\n",
      "Epoch 893, Training Loss:  tensor(62.3574, grad_fn=<SqrtBackward0>)\n",
      "Epoch 894, Training Loss:  tensor(111.1513, grad_fn=<SqrtBackward0>)\n",
      "Epoch 895, Training Loss:  tensor(120.2872, grad_fn=<SqrtBackward0>)\n",
      "Epoch 896, Training Loss:  tensor(110.8068, grad_fn=<SqrtBackward0>)\n",
      "Epoch 897, Training Loss:  tensor(137.6669, grad_fn=<SqrtBackward0>)\n",
      "Epoch 898, Training Loss:  tensor(346.0209, grad_fn=<SqrtBackward0>)\n",
      "Epoch 899, Training Loss:  tensor(524.7536, grad_fn=<SqrtBackward0>)\n",
      "Epoch 900, Training Loss:  tensor(241.8606, grad_fn=<SqrtBackward0>)\n",
      "Epoch 901, Training Loss:  tensor(255.0457, grad_fn=<SqrtBackward0>)\n",
      "Epoch 902, Training Loss:  tensor(136.9975, grad_fn=<SqrtBackward0>)\n",
      "Epoch 903, Training Loss:  tensor(143.7605, grad_fn=<SqrtBackward0>)\n",
      "Epoch 904, Training Loss:  tensor(143.3306, grad_fn=<SqrtBackward0>)\n",
      "Epoch 905, Training Loss:  tensor(162.7025, grad_fn=<SqrtBackward0>)\n",
      "Epoch 906, Training Loss:  tensor(306.1812, grad_fn=<SqrtBackward0>)\n",
      "Epoch 907, Training Loss:  tensor(238.4960, grad_fn=<SqrtBackward0>)\n",
      "Epoch 908, Training Loss:  tensor(160.8761, grad_fn=<SqrtBackward0>)\n",
      "Epoch 909, Training Loss:  tensor(170.3813, grad_fn=<SqrtBackward0>)\n",
      "Epoch 910, Training Loss:  tensor(214.4267, grad_fn=<SqrtBackward0>)\n",
      "Epoch 911, Training Loss:  tensor(76.0867, grad_fn=<SqrtBackward0>)\n",
      "Epoch 912, Training Loss:  tensor(133.6992, grad_fn=<SqrtBackward0>)\n",
      "Epoch 913, Training Loss:  tensor(300.6934, grad_fn=<SqrtBackward0>)\n",
      "Epoch 914, Training Loss:  tensor(84.3562, grad_fn=<SqrtBackward0>)\n",
      "Epoch 915, Training Loss:  tensor(133.0818, grad_fn=<SqrtBackward0>)\n",
      "Epoch 916, Training Loss:  tensor(144.7778, grad_fn=<SqrtBackward0>)\n",
      "Epoch 917, Training Loss:  tensor(228.5307, grad_fn=<SqrtBackward0>)\n",
      "Epoch 918, Training Loss:  tensor(408.4427, grad_fn=<SqrtBackward0>)\n",
      "Epoch 919, Training Loss:  tensor(156.5748, grad_fn=<SqrtBackward0>)\n",
      "Epoch 920, Training Loss:  tensor(66.3314, grad_fn=<SqrtBackward0>)\n",
      "Epoch 921, Training Loss:  tensor(52.7816, grad_fn=<SqrtBackward0>)\n",
      "Epoch 922, Training Loss:  tensor(126.3634, grad_fn=<SqrtBackward0>)\n",
      "Epoch 923, Training Loss:  tensor(155.2084, grad_fn=<SqrtBackward0>)\n",
      "Epoch 924, Training Loss:  tensor(140.7302, grad_fn=<SqrtBackward0>)\n",
      "Epoch 925, Training Loss:  tensor(226.9732, grad_fn=<SqrtBackward0>)\n",
      "Epoch 926, Training Loss:  tensor(252.7329, grad_fn=<SqrtBackward0>)\n",
      "Epoch 927, Training Loss:  tensor(182.4037, grad_fn=<SqrtBackward0>)\n",
      "Epoch 928, Training Loss:  tensor(135.7726, grad_fn=<SqrtBackward0>)\n",
      "Epoch 929, Training Loss:  tensor(175.1698, grad_fn=<SqrtBackward0>)\n",
      "Epoch 930, Training Loss:  tensor(172.9812, grad_fn=<SqrtBackward0>)\n",
      "Epoch 931, Training Loss:  tensor(225.9797, grad_fn=<SqrtBackward0>)\n",
      "Epoch 932, Training Loss:  tensor(177.6039, grad_fn=<SqrtBackward0>)\n",
      "Epoch 933, Training Loss:  tensor(185.7535, grad_fn=<SqrtBackward0>)\n",
      "Epoch 934, Training Loss:  tensor(213.2430, grad_fn=<SqrtBackward0>)\n",
      "Epoch 935, Training Loss:  tensor(416.8530, grad_fn=<SqrtBackward0>)\n",
      "Epoch 936, Training Loss:  tensor(226.2206, grad_fn=<SqrtBackward0>)\n",
      "Epoch 937, Training Loss:  tensor(360.2607, grad_fn=<SqrtBackward0>)\n",
      "Epoch 938, Training Loss:  tensor(410.4981, grad_fn=<SqrtBackward0>)\n",
      "Epoch 939, Training Loss:  tensor(246.3194, grad_fn=<SqrtBackward0>)\n",
      "Epoch 940, Training Loss:  tensor(152.5892, grad_fn=<SqrtBackward0>)\n",
      "Epoch 941, Training Loss:  tensor(133.5569, grad_fn=<SqrtBackward0>)\n",
      "Epoch 942, Training Loss:  tensor(201.3259, grad_fn=<SqrtBackward0>)\n",
      "Epoch 943, Training Loss:  tensor(245.6608, grad_fn=<SqrtBackward0>)\n",
      "Epoch 944, Training Loss:  tensor(228.0901, grad_fn=<SqrtBackward0>)\n",
      "Epoch 945, Training Loss:  tensor(70.4931, grad_fn=<SqrtBackward0>)\n",
      "Epoch 946, Training Loss:  tensor(189.1323, grad_fn=<SqrtBackward0>)\n",
      "Epoch 947, Training Loss:  tensor(522.3235, grad_fn=<SqrtBackward0>)\n",
      "Epoch 948, Training Loss:  tensor(145.2639, grad_fn=<SqrtBackward0>)\n",
      "Epoch 949, Training Loss:  tensor(281.7547, grad_fn=<SqrtBackward0>)\n",
      "Epoch 950, Training Loss:  tensor(319.9760, grad_fn=<SqrtBackward0>)\n",
      "Epoch 951, Training Loss:  tensor(312.4665, grad_fn=<SqrtBackward0>)\n",
      "Epoch 952, Training Loss:  tensor(128.5573, grad_fn=<SqrtBackward0>)\n",
      "Epoch 953, Training Loss:  tensor(214.9429, grad_fn=<SqrtBackward0>)\n",
      "Epoch 954, Training Loss:  tensor(58.1426, grad_fn=<SqrtBackward0>)\n",
      "Epoch 955, Training Loss:  tensor(153.0912, grad_fn=<SqrtBackward0>)\n",
      "Epoch 956, Training Loss:  tensor(195.0953, grad_fn=<SqrtBackward0>)\n",
      "Epoch 957, Training Loss:  tensor(219.6673, grad_fn=<SqrtBackward0>)\n",
      "Epoch 958, Training Loss:  tensor(65.2247, grad_fn=<SqrtBackward0>)\n",
      "Epoch 959, Training Loss:  tensor(267.4979, grad_fn=<SqrtBackward0>)\n",
      "Epoch 960, Training Loss:  tensor(100.3933, grad_fn=<SqrtBackward0>)\n",
      "Epoch 961, Training Loss:  tensor(94.4833, grad_fn=<SqrtBackward0>)\n",
      "Epoch 962, Training Loss:  tensor(182.6448, grad_fn=<SqrtBackward0>)\n",
      "Epoch 963, Training Loss:  tensor(55.5287, grad_fn=<SqrtBackward0>)\n",
      "Epoch 964, Training Loss:  tensor(44.4166, grad_fn=<SqrtBackward0>)\n",
      "Epoch 965, Training Loss:  tensor(261.5991, grad_fn=<SqrtBackward0>)\n",
      "Epoch 966, Training Loss:  tensor(159.3047, grad_fn=<SqrtBackward0>)\n",
      "Epoch 967, Training Loss:  tensor(63.0799, grad_fn=<SqrtBackward0>)\n",
      "Epoch 968, Training Loss:  tensor(48.5131, grad_fn=<SqrtBackward0>)\n",
      "Epoch 969, Training Loss:  tensor(184.9557, grad_fn=<SqrtBackward0>)\n",
      "Epoch 970, Training Loss:  tensor(82.1961, grad_fn=<SqrtBackward0>)\n",
      "Epoch 971, Training Loss:  tensor(141.3411, grad_fn=<SqrtBackward0>)\n",
      "Epoch 972, Training Loss:  tensor(225.4510, grad_fn=<SqrtBackward0>)\n",
      "Epoch 973, Training Loss:  tensor(111.1495, grad_fn=<SqrtBackward0>)\n",
      "Epoch 974, Training Loss:  tensor(215.5264, grad_fn=<SqrtBackward0>)\n",
      "Epoch 975, Training Loss:  tensor(200.7735, grad_fn=<SqrtBackward0>)\n",
      "Epoch 976, Training Loss:  tensor(77.0615, grad_fn=<SqrtBackward0>)\n",
      "Epoch 977, Training Loss:  tensor(66.6575, grad_fn=<SqrtBackward0>)\n",
      "Epoch 978, Training Loss:  tensor(205.6931, grad_fn=<SqrtBackward0>)\n",
      "Epoch 979, Training Loss:  tensor(80.2478, grad_fn=<SqrtBackward0>)\n",
      "Epoch 980, Training Loss:  tensor(244.5735, grad_fn=<SqrtBackward0>)\n",
      "Epoch 981, Training Loss:  tensor(266.0063, grad_fn=<SqrtBackward0>)\n",
      "Epoch 982, Training Loss:  tensor(79.3037, grad_fn=<SqrtBackward0>)\n",
      "Epoch 983, Training Loss:  tensor(238.2830, grad_fn=<SqrtBackward0>)\n",
      "Epoch 984, Training Loss:  tensor(124.1973, grad_fn=<SqrtBackward0>)\n",
      "Epoch 985, Training Loss:  tensor(93.7290, grad_fn=<SqrtBackward0>)\n",
      "Epoch 986, Training Loss:  tensor(154.6837, grad_fn=<SqrtBackward0>)\n",
      "Epoch 987, Training Loss:  tensor(175.6212, grad_fn=<SqrtBackward0>)\n",
      "Epoch 988, Training Loss:  tensor(322.8213, grad_fn=<SqrtBackward0>)\n",
      "Epoch 989, Training Loss:  tensor(53.4359, grad_fn=<SqrtBackward0>)\n",
      "Epoch 990, Training Loss:  tensor(179.0125, grad_fn=<SqrtBackward0>)\n",
      "Epoch 991, Training Loss:  tensor(109.0701, grad_fn=<SqrtBackward0>)\n",
      "Epoch 992, Training Loss:  tensor(113.1673, grad_fn=<SqrtBackward0>)\n",
      "Epoch 993, Training Loss:  tensor(35.5456, grad_fn=<SqrtBackward0>)\n",
      "Epoch 994, Training Loss:  tensor(125.0419, grad_fn=<SqrtBackward0>)\n",
      "Epoch 995, Training Loss:  tensor(106.2236, grad_fn=<SqrtBackward0>)\n",
      "Epoch 996, Training Loss:  tensor(148.0467, grad_fn=<SqrtBackward0>)\n",
      "Epoch 997, Training Loss:  tensor(107.7857, grad_fn=<SqrtBackward0>)\n",
      "Epoch 998, Training Loss:  tensor(130.4345, grad_fn=<SqrtBackward0>)\n",
      "Epoch 999, Training Loss:  tensor(165.6622, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1000, Training Loss:  tensor(92.7961, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MLPTorch()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "trainloader = torch.utils.data.DataLoader(dataset,batch_size=1024,shuffle=True)\n",
    "for epoch in range(1000):\n",
    "    for batch in trainloader:\n",
    "        x,y = batch\n",
    "        preds = model(x)\n",
    "        loss = criterion(y,preds).sqrt()\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: \",loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27.6593], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './polynomial.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('imageproc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbf8f6931e593c1d20f304ee88c30b2bf8792293dec0efad3793d777ab79c16d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
